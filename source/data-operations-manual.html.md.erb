---
title: Data Operations Manual
weight: 30
---

# Data Operations Manual

The planning data service relies upon a set of data workflows. The main
goal is to collect data from external organisations published in accordance with data specifications published by
digital land and make it available as part of a national dataset.

## Key Processes

This is our list of key processes that the team will need to perform to maintain the service. They primarily rely on running CLI tools from our
digital-land-python python package

### CLI Command: add_endpoint_and_lookups_cmd

#### 1. Checkout repo
From Github, checkout the Collection repo you wish to add endpoints to.
Make a note of the location [COLL-DIR].

#### 2. run collection init
Run the following lines from a virtual env containing the required
python dependencies required by the collection repo
```
make makerules
make init
```

#### 3. get your csv of endpoints to add
Find or generate a csv file of the entries that need to be added
to the collection. Make a note of the location [CSV-PATH] / *FILE_NAME*.csv].

#### 4. run add_endpoint_and_lookups_cmd
Run the following line from the **digital-land-python** repository, using a virtual env containing the required
python dependencies required by **digital-land-python**
(line broken up for readability)
```
-n [DATASET-NAME] \
-p [COLL-DIR]/pipeline \
-s [SPECIFICATION-DIR] \
add_endpoint_and_lookups_cmd \
[CSV-PATH]/[FILE_NAME].csv] \
-c [COLL-DIR]/collection \
-o [ORGANISATION-DIR]
```

#### 5. check assigned entities are normal
After running the command, the following Collection repo
files will be modified:

```
collection/source.csv
collection/endpoints.csv
pipeline/lookup.csv
```

The console output will show a list of new lookup entries
organised by organisation and resource-hash.
E.g.
```
----------------------------------------------------------------------
>>> organisations:['local-authority-eng:ARU']
>>> resource:6c38cd1f84054051ca200d62e9715be0cd739bedbae0db9561ef091fa95f59f1
----------------------------------------------------------------------
brownfield-land,,local-authority-eng:ARU,BR23911,1729345
brownfield-land,,local-authority-eng:ARU,BR19811,1729346
...
```
Find the first lookup entry in the console output,
make a note of the entity id (the number at the end),
and find this in pipeline/lookup.csv.

Check this and all subsequent lines in lookup.csv for
any anomalies. Each record should have, as a minimum, a prefix,
organisation and reference.

#### 6. run pipeline
Run the following line from a virtual env containing the required
python dependencies required by the collection repo
```
make collect
```

#### 7. perform final checks of the data for any anomalies
How this step is performed will largely rely on the level of the operator's
system knowledge.
One approach might be to identify a new (or newly updated) entry in the
.csv file, and search for this entry in the digital_land website.
Navigating through to the Entity screen will show information that may
be compared to the data in the collection/source.csv
