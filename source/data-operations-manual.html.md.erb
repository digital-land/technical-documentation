---
title: Data Operations Manual
weight: 30
---

# Data Operations Manual

The planning data service relies upon a set of data workflows. The main
goal is to collect data from external organisations published in accordance with data specifications published by
digital land and make it available as part of a national dataset.

## Key Processes

This is our list of key processes that the team will need to perform to maintain the service. They primarily rely on running CLI tools from our
digital-land-python python package. 

### Adding Endpoints To A Collection Manually

This is a fairly common process for us especially as we initially grow the data on the site.

#### 1. Checkout repo

From Github, clone the collection repo you wish to add endpoints to.

#### 2. run collection init

Run the following lines from a virtual env containing the required
python dependencies required by the collection repo

```bash
make makerules
make init
```

#### 3. get your csv of endpoints to add

Find or generate a csv file of the entries that need to be added
to the collection. Make a note of the location. The following columns need to be included in the csv:

- `endpoint-url` - the url that the collector needs to extract data from
- `documentation-url` - a url on a government related domain which contain information regarding the data
- `start-date` - the date that the collector should start from (this can be in the past)
- `plugins` - if a plugin is required to extract that data then it can be noted here othwerwise leave blank
- `pipelines` - the pipelines that need to be ran on resources collected from this endpoint. These are equivalent to the datasets and where more than one is neccessaary they should be separated by `;`
- `organisation` - the organisation which the endpoint belongs to. The name should be in [this list](https://datasette.planning.data.gov.uk/digital-land/organisation)

If you haven't been given some of the above information then reach out to the data manager or tech lead for advise. 

#### 4. run `add_endpoint_and_lookups_cmd`

Run the following command from inside the repository with the virtual env created in step 2 activated.
As a minimum you'll need to include the path to the input csv generated in step 3. and the name of the collection that your working in.

Note: collection name can be retrieved from the repository name. if your working in the `conservation-area-collection` repo then you would use `conservation-area` 

```
digital_land add-endpoints-and-lookups [INPUT-CSV-PATH] [COLLECTION_NAME]
```

The above command will work for the default collection set-up however you can set set the location
of the key dependenciess if any of the directories or files are in a different location.
this is unnessary the majority of the time

```
-c, --collection-dir [COLLECTION-DIRECTORY; default="/collection"] The directory containing the collection configuration files, collection logs and resources
-o, --organisation-path [ORGANISATION-CSV-PATH; default="var/cache/organisation.csv"] The path to the csv containing the list of organisations
-s, --specification-dir [SPECIFICATION-DIR; default="/specification"] The directory containing the specification files
-p, --pipeline-dir [PIPELINE-DIRECTORY; default="/pipeline"] The direcotry contianing pipeline configuration files
```

#### 5. check assigned entities are normal
After running the command, the following Collection repo
files will be modified:

```
collection/source.csv
collection/endpoints.csv
pipeline/lookup.csv
```

The console output will show a list of new lookup entries
organised by organisation and resource-hash.
E.g.

```
----------------------------------------------------------------------
>>> organisations:['local-authority-eng:ARU']
>>> resource:6c38cd1f84054051ca200d62e9715be0cd739bedbae0db9561ef091fa95f59f1
----------------------------------------------------------------------
brownfield-land,,local-authority-eng:ARU,BR23911,1729345
brownfield-land,,local-authority-eng:ARU,BR19811,1729346
...
```

Find the first lookup entry in the console output,
make a note of the entity id (the number at the end),
and find this in pipeline/lookup.csv.

Check this and all subsequent lines in lookup.csv for
any anomalies. Each record should have, as a minimum, a prefix,
organisation and reference.

#### 6. run pipeline

Run the following line from a virtual env containing the required
python dependencies required by the collection repo

```
make
```

#### 7. perform final checks of the data for any anomalies
How this step is performed will largely rely on the level of the operator's
system knowledge.
One approach might be to identify a new (or newly updated) entry in the
.csv file, and search for this entry in the digital_land website.
Navigating through to the Entity screen will show information that may
be compared to the data in the collection/source.csv

Once the pipeline is ran you can also use the following command to interrogate 
the local datasets (in the sqlite files) using datasette

### Creating a new collection

This is needed when adding a dataset that doesn't belong to one of the collections
that already exist.

#### 1. Ensure the collection is noted in the specification

Not doing this won't result in any specific errors but it should be done to ensure that the list of collections in the specification is up to date. This should be done with some visiability from the standards team

#### 2. Use the [collection-template](https://github.com/digital-land/collection-template) to create a repository

The `collection-template` repo is a template repository which can be used to create
new collection repos. This is fairly simple using the Github UI. there is an option
to use the template to create a new repo if you navigate to the `collection-template`
page

#### 3. Update `.README`

Update the details in the readme file.

#### 4. Deactivate actions

The collection comes with the action ready to run every night. This should be paused until
your ready for it to be ran. This can be done by commenting out the following lines in the  `.github
/workflows/run_caller.yaml` :

```
schedule:
    - cron: 0 0 * * *
```

#### 5. Add endpoints, edit pipeline config files and test locally.

This is where you need to get the pipeline running locally. see our other 
processes on this page for how to add data 

#### 6. Re-activate actions

Once the collection is ready to be ran nightly uncomment the line from above in 
the git workflow yaml file and it will start running nightly!


---
**Process Review**

Everything else is done so it's worth reviewing this process and seeing if there's
anyway we can improve it!

---


### Adding a new dataset

These requests will likely be few and far between but will happen and will require 
co-ordination between both the operations and standards teams. Requests will likely 
come from both the data manager and the standards team.

#### 1. Ensure the dataset has been added to the specification

Before resources can be processed for a dataset the dataset itself needs to be registered
in the specification. This includes recording which fields are needed at the end of the pipeline
and the entity number range.

This should be done by the standards team before datasets are requested to come
to the platform but if not it's worth involving them here before you continue.

Note: Don't include a collection in the dataset specification until the dataset set should go
on the site. this will allow you to run the pipeline and push data to s3 without
it being added to the platform

#### 2. Identify or create a collection

A dataset HAS to be added to a collection repository. If there isn't already one
then view the process to add a collection and follow the initial steps.

#### 3. Identify and add an endpoint for the dataset

Once the above steps have been taken the only thing needed to be done is to add endpoints for the dataset. this can be done using our add endpoints process.

Note: if you are adding a dataset that is 'hosted' by us then it can be put in a data folder of the repo and the raw github link e.g. `https://raw.githubusercontent.com/digital-land/ancient-woodland-collection/main/data/ancient-woodland-status.csv` can then be used as an endpoint.

#### 4. Test locally and in GHA

it's worth testing the dataset locally and in the GHA. this ensures that everything is working as expected. configuratino files can be altered to ensure it works as expected.

#### 5. Add the collection to the dataset in the specification

This will signal that the dataset should be loaded into the postgresql DB behind the platform. once this step is done when the collection is ran it'll upload to the platform.

---
**Process Review**

Everything else is done so it's worth reviewing this process and seeing if there's
anyway we can improve it!

---

## Technical Glossary

We use a lot of terms that are very specific to our system. Sometimes these also vary based on context so this is an attempt
to build a collective understanding in one place.

|Term|Definition|
|:----|:----|
|Collection Repository| This refers to one of our 30+ repos in GitHub which contain all of the configuration to run a collection each night.|
|Collection Directory|This is different to a collection repo and is used to ddescribe the colleciton directory in each of the collection repos which contian both the config files for a collector to run and also the logs and collected resources from previous collector runs.|

