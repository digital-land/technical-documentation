---
title: Data Operations Manual - Key Processes - Adding An Endpoint
weight: 40102
---

# Adding An Endpoint

---
:warning: before adding an endpoint it's worth validating the endpoint see our process [here](/key-processes/validate-endpoint.html)
---

---
:warning: We are currently attempting to centralise both pipeline and collection configuration files. There are
two sets of instructions below depending on whether we have centralised the files for the collection you are 
altering. See [here](/centralising-config/index.html) for a list of collections that we are working on.
---

### Adding Endpoints To A Non-centralised Collection

This is a fairly common process for us especially as we initially grow the data on the site.

#### 1. Checkout repo

From Github, clone the collection repo you wish to add endpoints to.

#### 2. run collection init

Run the following lines from a virtual env containing the required
python dependencies required by the collection repo

```bash
make makerules
make init
```

#### 3. get your csv of endpoints to add

---
:warning: You can get a copy of this csv generated for you by validating the endpoint
---

Find or generate a csv file of the entries that need to be added
to the collection. Make a note of the location. The following columns need to be included in the csv:

- `endpoint-url` - the url that the collector needs to extract data from
- `documentation-url` - a url on a government related domain which contain information regarding the data
- `start-date` - the date that the collector should start from (this can be in the past)
- `plugins` - if a plugin is required to extract that data then it can be noted here otherwise leave blank
- `pipelines` - the pipelines that need to be ran on resources collected from this endpoint. These are equivalent to the datasets and where more than one is necessary they should be separated by `;`
- `organisation` - the organisation which the endpoint belongs to. The name should be in [this list](https://datasette.planning.data.gov.uk/digital-land/organisation)

If you haven't been given some of the above information then reach out to the data manager or tech lead for advice. You can use the below to get you started:

```text
organisation,endpoint-url,documentation-url,start-date,pipelines,plugin
```

For example 

```text
organisation,endpoint-url,documentation-url,start-date,pipelines,plugin
local-authority-eng:GRT,https://www.guildford.gov.uk/media/35723/Guildford-Brownfield-Register-2023/CSV/guildford_brownfieldregister_2023_10_12_rev1.csv?m=638327116385130000,https://www.guildford.gov.uk/article/25473/What-is-the-register,2023-10-12,brownfield-land,
```

#### 4. run `add_endpoint_and_lookups_cmd`

---
:warning: The Below command can be generated by validating the endpoint
---

Run the following command from inside the repository with the virtual env created in step 2 activated.

As a minimum you'll need to include the path to the input csv generated in step 3. and the name of the collection that your working in.

Note: collection name can be retrieved from the repository name. if your working in the `conservation-area-collection` repo then you would use `conservation-area`

```
digital_land add-endpoints-and-lookups [INPUT-CSV-PATH] [COLLECTION_NAME]
```

The above command will work for the default collection set-up however you can set set the location
of the key dependencies if any of the directories or files are in a different location. This should not be necessary most of the time.

```
-c, --collection-dir [COLLECTION-DIRECTORY; default="/collection"] The directory containing the collection configuration files, collection logs and resources
-o, --organisation-path [ORGANISATION-CSV-PATH; default="var/cache/organisation.csv"] The path to the csv containing the list of organisations
-s, --specification-dir [SPECIFICATION-DIR; default="/specification"] The directory containing the specification files
-p, --pipeline-dir [PIPELINE-DIRECTORY; default="/pipeline"] The directory containing pipeline configuration files
```

#### 5. check assigned entities are normal
After running the command, the following Collection repo
files will be modified:

```
collection/source.csv
collection/endpoints.csv
pipeline/lookup.csv
```

The console output will show a list of new lookup entries
organised by organisation and resource-hash.
E.g.

```
----------------------------------------------------------------------
>>> organisations:['local-authority-eng:ARU']
>>> resource:6c38cd1f84054051ca200d62e9715be0cd739bedbae0db9561ef091fa95f59f1
----------------------------------------------------------------------
brownfield-land,,local-authority-eng:ARU,BR23911,1729345
brownfield-land,,local-authority-eng:ARU,BR19811,1729346
...
```

Find the first lookup entry in the console output,
make a note of the entity id (the number at the end),
and find this in pipeline/lookup.csv.

Check this and all subsequent lines in lookup.csv for
any anomalies. Each record should have, as a minimum, a prefix,
organisation and reference.

#### 6. run pipeline

Run the following line from a virtual env containing the required
python dependencies required by the collection repo

```
make
```

#### 7. perform final checks of the data for any anomalies
How this step is performed will largely rely on the level of the operator's
system knowledge.
One approach might be to identify a new (or newly updated) entry in the
.csv file, and search for this entry in the digital_land website.
Navigating through to the Entity screen will show information that may
be compared to the data in the collection/source.csv

Once the pipeline is ran you can also use the following command to interrogate
the local datasets (in the sqlite files) using datasette

```
make datasette
```

### Adding Endpoints To A Centralised Collection

This is a fairly common process for us especially as we initially grow the data on the site.

#### 1. Checkout repo

From Github, clone the [config repo](https://github.com/digital-land/config).

#### 2. run collection init

Run the following lines from a virtual env containing the required
python dependencies required by the collection repo

```bash
make init
```

#### 3. get your csv of endpoints to add

---
:warning: You can get a copy of this csv generated for you by validating the endpoint
---

Find or generate a csv file of the entries that need to be added
to the collection. Make a note of the location. The following columns need to be included in the csv:

- `endpoint-url` - the url that the collector needs to extract data from
- `documentation-url` - a url on a government related domain which contain information regarding the data
- `start-date` - the date that the collector should start from (this can be in the past)
- `plugins` - if a plugin is required to extract that data then it can be noted here otherwise leave blank
- `pipelines` - the pipelines that need to be ran on resources collected from this endpoint. These are equivalent to the datasets and where more than one is necessary they should be separated by `;`
- `organisation` - the organisation which the endpoint belongs to. The name should be in [this list](https://datasette.planning.data.gov.uk/digital-land/organisation)

If you haven't been given some of the above information then reach out to the data manager or tech lead for advice. You can use the below to get you started:

```text
organisation,endpoint-url,documentation-url,start-date,pipelines,plugin
```

For example 

```text
organisation,endpoint-url,documentation-url,start-date,pipelines,plugin
local-authority-eng:GRT,https://www.guildford.gov.uk/media/35723/Guildford-Brownfield-Register-2023/CSV/guildford_brownfieldregister_2023_10_12_rev1.csv?m=638327116385130000,https://www.guildford.gov.uk/article/25473/What-is-the-register,2023-10-12,brownfield-land,
```

#### 4. run `add_endpoint_and_lookups_cmd` in the config repo

Run the following command from inside the repository with the virtual env created in step 2 activated.

As a minimum you'll need to include the path to the input csv generated in step 3. and the name of the collection that your working in.

Note: collection name can be retrieved from the repository name. if your working in the `conservation-area-collection` repo then you would use `conservation-area`

```
digital_land add-endpoints-and-lookups [INPUT-CSV-PATH] [COLLECTION_NAME] -c ./collection/[COLLECTION_NAME] -p ./pipeline/[COLLECTION_NAME]
```

The above command will work for the config set-up however you can set set the location
of the key dependencies if any of the directories or files are in a different location. This should not be necessary most of the time.

```
-c, --collection-dir [COLLECTION-DIRECTORY; default="/collection"] The directory containing the collection configuration files, collection logs and resources
-o, --organisation-path [ORGANISATION-CSV-PATH; default="var/cache/organisation.csv"] The path to the csv containing the list of organisations
-s, --specification-dir [SPECIFICATION-DIR; default="/specification"] The directory containing the specification files
-p, --pipeline-dir [PIPELINE-DIRECTORY; default="/pipeline"] The directory containing pipeline configuration files
```

#### 5. check assigned entities are normal
After running the command, the following Collection repo
files will be modified:

```
collection/[COLLECTION_NAME]/source.csv
collection/[COLLECTION_NAME]/endpoint.csv
pipeline/[COLLECTION_NAME]/lookup.csv
```

The console output will show a list of new lookup entries
organised by organisation and resource-hash.
E.g.

```
----------------------------------------------------------------------
>>> organisations:['local-authority-eng:ARU']
>>> resource:6c38cd1f84054051ca200d62e9715be0cd739bedbae0db9561ef091fa95f59f1
----------------------------------------------------------------------
brownfield-land,,local-authority-eng:ARU,BR23911,1729345
brownfield-land,,local-authority-eng:ARU,BR19811,1729346
...
```

Find the first lookup entry in the console output,
make a note of the entity id (the number at the end),
and find this in `pipeline/[COLLECTION_NAME]/lookup.csv.`

Check this and all subsequent lines in lookup.csv for
any anomalies. Each record should have, as a minimum, a prefix,
organisation and reference.

#### 6. Push Changes

Use git to push changes up to the repository, each night when the collection runs the files are downloaded
from here. It is a good idea to name the commit after the organisation you are importing.

#### 7. run pipeline - optional

Checkout and set-up the relevant colleciton repo. the CONFIG URL can be changed
if you with to run with a branch of of the config repo.

Once setup up the pipeline can be ran in the colleciton repo by running:

```
make
```

After it has completed you can investigate the results.
One approach might be to identify a new (or newly updated) entry in the
.csv file, and search for this entry in the digital_land website.
Navigating through to the Entity screen will show information that may
be compared to the data in the collection/source.csv

Once the pipeline is ran you can also use the following command to interrogate
the local datasets (in the sqlite files) using datasette


```
make datasette
```