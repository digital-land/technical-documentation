---
title: Data Operations Manual - Key Processes - Validating An Endpoint
weight: 40101
---

# Validating an Endpoint

--
:warning: We are currently attempting to centralise both pipeline and collection configuration files. this does not
affect the current validation process. See for a list of collections that we are working on.
---

The main way to validate endpoints is by using the endpoint checker. This is a jupyter notebook in the [jupyter analysis repo](https://github.com/digital-land/jupyter-analysis).
There is also the [publish tool](https://publish.staging.planning.data.gov.uk) (staging version to let LPAs use the production one).

# Endpoint Checker

Most steps are explained in the notebook itself but it will require downloading the repo, and creating the right python venv including installing packages

You need to know:

* The collection you are checking against
* The organisation code for the [data publisher](https://datasette.planning.data.gov.uk/digital-land?sql=select%0D%0A++entity%2C%0D%0A++name%2C%0D%0A++organisation%2C%0D%0A++website%0D%0Afrom%0D%0A++organisation%0D%0Aorder+by%0D%0A++organisation%0D%0Alimit%0D%0A++1000)
* The URL where their documentation is (optional but **highly** recommended)
* The URL where the data lives
* what data it is you are getting
* The start_date for this data. This can be in the future, for checking at least.

If the file is a CSV, download it first and have a look to make sure that it meets the basic sanity-checks:

* Is there a reference value for every row?
* Are the mandatory fields for that Collection provided?
* Are there any formatting problems, like multi-line fields (eg addresses) or misplaced commas?
* Are there any rows at the end of the data that are going to cause problems? There is sometimes text or other content that isn't going to work for us.

If the link is for a geographic system, again download it and have a look at the data to ensure that there's a reference field and that the mandatory fields are present. You can use the guidance in [Working with LPA GIS Systems](WorkingWithLPA_GIS.html) to help deal with any issues. A common one is that the publisher might provide a query with a list of output fields. These have commas in so tends to break the import process. You can usually safely replace all the fields with a single * character to get all the fields.

The endpoint checker gives you a lot of information about the chances of successfully loading the data. I suggest you pick some URLs from existing endpoint.csv files and try them out to get a flavour of what it does but in general you will see

1. Can we get the data downloaded? If so, you'll see information about the size of the download etc
1. Can we convert it to a CSV structure?
1. Can we match the columns provided with what we expect according to the standard for that Collection? In the past we have been quite lenient with column mapping for LPAs. As we move towards a more standardised approach, publishers should be using the standards.
1. Does our system generate lookup values that make sense for the data given?
1. Are we seeing the right number of rows being brought in vs the original input?
1. Does the final representation of what the imported data look right? 

If the data passes those tests you can move on to loading it onto the platform, below.

### Loading data onto the platform 

If you're happy with the results of validating your new endpoint you can use the final part of the notebook to find some convenient scripts to help set up the new download. see the process for adding an endpoint.
