---
title: Run Book
weight: 10
---

# Run Book

This document contains some basic instructions for fixing common issues encountered operating the service.

## Outage Procedure

When an outage occurs the following process should be followed.

* Post a message into the team chat in slack
  * Replace **AFFECTED_APPLICATIONS** with a list of services that are down
  * Replace **MEET_LINK** with a google meet link [create a new one here](https://meet.new)
  * Replace **INCIDENT_RESPONSE_DOCUMENT** with a google doc link [create a new one here](https://doc.new)

```
@here *Service Outage*

AFFECTED_APPLICATIONS

MEET_LINK

INCIDENT_RESPONSE_DOCUMENT
```

* All team members must join the google meet link and begin documenting the outage.

Resolving an outage is an iterative process that should be improved with the lessons learnt from the previous outages.
It is suggested that the outage document contain the following headers to direct work appropriately.

* **Outage - APPLICATION - DATE (YYYY-MM-DD format)**
* **In attendance** - a list of people who have responded to the outage and the roles they have taken.
* **Description** - A brief description of the outage, including affected services
* **Running log** - Put timestamped entries under here including actions, observations, etc.
    * Action **TIME We did x.**
    * Observation **TIME We observed that x happened.**
* **Postmortem** - A few paragraphs explaining the root cause of the issue and what measures have been taken to mitigate
further instances.

Outage Roles:

* **Scribe** The role of scribe is to maintain the incident report document during diagnosis and resolution of the outage.
  * Requires edit access to the google document tracking the incident.
  * Requires read-write access to the technical documentation repository.
* **Observer** The role of observer is to keep track of changes made to application and infrastructure code, approving
    any pull requests where needed.
  * Requires read-only access to the AWS account hosting the applications that are experiencing an outage.
  * Requires read-write access to any GitHub repositories hosting code for the applications that are experiencing an outage.
* **Actor** The role of actor is to be the only person who makes changes to application code and deployed infrastructure.
  * Requires read-write access to the AWS account hosting the applications that are experiencing an outage.
  * Requires read-write access to any GitHub repositories hosting code for the applications that are experiencing an outage.

It is worth checking the [incident response history](#incident-response-history)

Once the outage has been resolved it is a good idea to take a break before reflecting on the outage and completing the
postmortem section of the document. Once the document is complete, the contents must be copied under the section
[Incident Response History](#incident-response-history) for future reference, taking care to remove any secret
information from the document.

## Common Issues


## Incident Response History

### All applications on planning.data.gov.uk - 2022-11-22

#### In attendance

Async on slack:

* Paul Downey
* Lawrence Goldstien
* Paul Smith

#### Description

On the morning of the 22nd the decision was taken to switch over to the new infrastructure setup in the production
account. We made preparations by adding a service outage banner to the application as we were aware that some downtime
would be experienced while DNS updated globally. After the switchover Lawrence was able to access the new infrastructure
so assumed everything was going to plan. At 2:45pm Paul Downey noted that the site was not accessible and diagnostic
procedures started at that point.

#### Running log

* GDS contact confirmed the DNS change had been actioned at 2:15pm
* Lawrence observed that the datasette and datasette tiles applications were available on the new domain at 2:20pm
* Lawrence observed that the main application was available at 2:35pm
* Paul Downey observed that the site was still not accessible for him and posted the results of a dig on the main
    application domain at 2:50pm
* Lawrence used the site https://www.whatsmydns.net/#NS/planning.data.gov.uk to check for propagation and noted that
    most DNS servers had updated to the new environment at 3pm
* Paul Downey observed that the website worked for him at 4pm
* Lawrence and Paul Downey observed that the site was working intermittently from 4pm to 6pm.

**NEXT DAY (2022-11-23)**

* Paul Downey observed that the site worked for him when tethered to his phone but saw that one in three runs of dig
    against the domain returned empty results at 9am
* Paul Downey posted into the digital planning slack to advise that we thought the change went through and to let the
    team know of any issues at 9am
* Lawrence observed that some DNS servers had longer TTLs for the domain compared to the current and old DNS records
    TTLs of 24 hours at 1pm
* The decision was made to wait out the full 48 hours of TTL and continue work to solve the issues afterwards at 1:30pm

**NEXT DAY (2022-11-24)**

* Paul Downey noted that the DNS responses where still not returned consistently when running a dig command at 8:30am
* Lawrence ran some diagnostics against a selection of public DNS servers and noted that the data.gov.uk domain has two
    sets of DNS servers: AWS and Google Cloud. Lawrence found that running a dig against AWS servers for the domain
    returned details for the new domain whereas running a dig against the Google Cloud returned no results at 10am
* Lawrence drafted a message to be sent to GDS detailing the issues with the Google Cloud nameservers at 10am
* GDS confirmed the issue and confirmed they had synced AWS and Google Cloud nameservers at 11:50am
* Paul Downey observed that the site was up and running for him at 11:55am
* Lawrence completed further checks and noted that most global DNS servers had updated to the new infrastructure at 12pm

#### Postmortem

We felt the team reacted well to this outage and stakeholders were understanding of the downtime.

Paul Smith developed a pattern for service status messages to users and added feedback to the
[GOV.UK Design system](https://github.com/alphagov/govuk-design-system-backlog/issues/266).

In reflection the team could have recognised the issues sooner and checked more details of the DNS configuration.

We could have offered the old address https://digital-land.info to OSL and other groups as a fallback while the system
was down.
