---
title: Data Operations Manual - Key Processes - Adding An Endpoint
weight: 40102
---

# Adding An Endpoint
### Checking Data before loading it onto the platform with Endpoint Checker

The [Endpoint Checker](https://github.com/digital-land/jupyter-analysis/tree/main/endpoint_checker) is a Jupyter notebook that you can use to get an idea of the level of usability of the data offered on the web by an LPA. The notebook summarises all the details related to an endpoint in one place, and is a good first place to look when validating a new endpoint. (This can still be used to validate endpoints however the code it will print has not been updated to work within the config repo.)

#### pre-requisites

You need to know 

* The collection you are checking against
* The organisation code for the [data publisher](https://datasette.planning.data.gov.uk/digital-land?sql=select%0D%0A++entity%2C%0D%0A++name%2C%0D%0A++organisation%2C%0D%0A++website%0D%0Afrom%0D%0A++organisation%0D%0Aorder+by%0D%0A++organisation%0D%0Alimit%0D%0A++1000)
* The URL where their documentation is (Now **mandatory**)
* The URL where the data lives
* what data it is you are getting
* The start_date for this data. This can be in the future, for checking at least.
* The license code for the endpoint

If the file is a CSV, download it first and have a look to make sure that it meets the basic sanity-checks:

* Is there a reference value for every row?
* Are the mandatory fields for that Collection provided?
* Are there any formatting problems, like multi-line fields (eg addresses) or misplaced commas?
* Are there any rows at the end of the data that are going to cause problems? There is sometimes text or other content that isn't going to work for us.

If the link is for a geographic system, again download it and have a look at the data to ensure that there's a reference field and that the mandatory fields are present. You can use the guidance in [Working with LPA GIS Systems](WorkingWithLPA_GIS.html) to help deal with any issues. A common one is that the publisher might provide a query with a list of output fields. These have commas in so tends to break the import process. You can usually safely replace all the fields with a single * character to get all the fields.

### Loading data onto the platform 

If you're happy with the results of validating your new endpoint you can use the final part of the notebook to find some convenient scripts to help set up the new download. (This has not been updated to give the correct information for the config repo)

1. Download the config repository
1. Set up and activate your virtual environment for it
1. Run `make makerules` and `make init`
1. Create an `import.csv` file per the notes in the Jupyter notebook (This remains valid however license will also be needed to be added as a column)
1. Copy the two lines from the notebook into the file
1. Save it and run the command line example from the notebook however it is important to add the -c and -p options at the end of the command which specify the directories for the collection and pipeline youâ€™re adding to. 
1.An example would look like: digital-land add-endpoints-and-lookups ./import.csv conservation-area -c ./collection/conservation-area -p ./pipeline/conservation-area

Depending on the amount of data present in the Collection, the import process may take some time. Once it completes, review the `source.csv`, `endpoint.csv` and `lookup.csv` files to make sure they make sense. 

#### Saving changes back

You can push the the `source.csv`, `endpoint.csv` and `lookup.csv` files files back to github, having discarded any changes to other files not needed but changed by the process. It is a good idea to name the commit after the organisation you are importing. I've been using the name of the organisation plus the three letter code.
