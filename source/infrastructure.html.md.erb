---
title: Infrastructure
weight: 20
---

# Infrastructure

The planning data [infrastructure repository](https://github.com/digital-land/digital-land-infrastructure) contains
terraform configuration files that create the necessary services to deploy applications and background tasks for the
planning data system.

## Overview

<% plantuml("c4-context.puml") %>

There are two repositories that handle our infrastructure:
* Digital Land Infrastructure - this contains our primary infrastructure and allows the user to interact all environments and is what the instructions in this guide aim to help with.
* Global Digital Land Infrastructure - this mainly contains infrastructure that needs to lie outside of the other repo such as nameserver records to subdomains as well as lagacy infrustructure such as s3 buckets who's content is stored in glacier

## Deploying

#### Authentication

[aws-vault](https://github.com/99designs/aws-vault) is recommended to assume the correct role for deploying resources to
the AWS environments. This guide does not cover the correct configuration of aws-vault.

Once configured correctly all the following commands can be executed as follows.

```shell
aws-vault exec <profile> -- <command>
```

### An Existing Environment

To deploy updates to an existing environment first, clone the
[infrastructure repository](https://github.com/digital-land/digital-land-infrastructure)
`git clone git@github.com:digital-land/digital-land-infrastructure.git`.

Once cloned, run `make apply STAGE=<environment>` to deploy the updated infrastructure configuration.

### A New Environment

To deploy a new environment the following details are needed:

* Environment name
* Root domain name

First, clone the [infrastructure repository](https://github.com/digital-land/digital-land-infrastructure)
`git clone git@github.com:digital-land/digital-land-infrastructure.git`.

Once cloned, run `make new-environment STAGE=<environment> DOMAIN=<root domain>` to create the needed configuration
files. Commit the created files to the repository.

Run `make apply STAGE=<environment>` to deploy the new environment, do not worry about errors, this will fail the first
time you deploy a new environment. There should be roughly 500 new resources created to action the deployment.

On completion of the first deployment run you will see errors regarding the verification of SSL certificates created for
the environment.

**First `terraform apply` output**

```shell
│ Error: error creating CloudFront Distribution: InvalidViewerCertificate: The specified SSL certificate doesn't exist, isn't in us-east-1 region, isn't valid, or doesn't include a valid certificate chain.
│       status code: 400, request id: xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx
│
│   with module.application_main.aws_cloudfront_distribution.cdn,
│   on modules/application/application_traffic.tf line 28, in resource "aws_cloudfront_distribution" "cdn":
│   28: resource "aws_cloudfront_distribution" "cdn" {
│
```

To resolve this error we must configure the DNS provider for the root domain to use these nameservers, and then run the
deployment for the environment a second time. To obtain the DNS nameservers, first log into the AWS console
(`aws-vault login <profile>`), then access Route53 > Hosted Zones > [domain] and get the four values for the NS record,
configuring these servers with the domain's DNS provider.

This can be done by using the global digital land infrastructure to create the required record.

Once DNS delegation is complete, you may check that the SSL certificate has been generated by logging into AWS for the
appropriate account and checking the ACM service in the us-east-1 region. The relevant certificate will show as verified
once delegation is complete.

You can now run `make apply STAGE=<environment>` for the second time and should see changes to the CDN and Route53
resources. The new infrastructure is now set up, and all that remains is to deploy applications into the new
environment. At this point all applications will serve a page stating that the service will shortly be resumed.

#### Deploying data

In order for the applications to start and run we must have data in the platform. Firstly we need both background tasks
deployed to the environment prior to uploading data for processing.

Deploy the following applications using the dispatch workflow

* [datasette-tiles (task)](https://github.com/digital-land/tiles-builder/actions/workflows/deploy-task.yml)
    A background task that builds [MBTiles](https://github.com/mapbox/mbtiles-spec) files for use in the datasette-tiles
    application.
* [postgres-importer](https://github.com/digital-land/digital-land-postgres/actions/workflows/deploy.yml)
    A background task that imports the digital-land.sqlite3 in a postgres database.

Once both applications are deployed we can now copy data into the environment. Data generally enters the environment via
upload as part of a [collection job](https://github.com/digital-land?q=-collection) or via the
[entity-builder](https://github.com/digital-land/entity-builder) and
[digital-land-builder](https://github.com/digital-land/digital-land-builder) jobs.

While these data pipelines run on a regular basis you may need to start with a set of data from another environment in
order to successfully deploy the various applications needed to run the system. The AWS command line application has a
command to make this easier. Run with `--dryrun` to preview the files to be synced (though the output is not useful).

```bash
aws s3 sync s3://[source]-collection-data s3://[destination]-collection-data
```
Note: may be worth deployed the main appliication once before deploying data and once after as the database schema is applied when the application is
    
#### Deploying applications

Now the new environment is up and running you should deploy the following applications:

* [main](https://github.com/digital-land/digital-land.info/actions/workflows/continuous-deployment.yml)
    The main digital land web application.
* [datasette](https://github.com/digital-land/datasette-builder/actions/workflows/deploy.yml)
    A web service and API allowing SQL queries against sqlite databases.
* [datasette-tiles (application)](https://github.com/digital-land/tiles-builder/actions/workflows/deploy-application.yml)
    A web service that allows the main application's frontend to draw vector tiles on the map feature from our datasets.

Note: data may need to be re-deployed to the postgres db once the application has been deployed due to the build failing if no data is found but the database schema only being deployed when the app is

## Configuration Changes

### Add a new application

To create a new application in the terraform configuration, you will need to know the following:

* The **GitHub repository** (under the [digital-land](https://github.com/digital-land) organisation) hosting the application code.
* The **subdomain** of `planning.data.gov.uk` that the application will run under.
* An estimate of the **memory usage** of the application.
* A list of the application **environment variables**.
* If the application needs access to the SQLite or MBTiles files generated by background tasks.

Add a new section to `variables.tf` under `locals.application_defaults`, replacing attributes as required.

```hcl
<name> = {
  // The subdomain to serve the application from
  access = {
    domain = "<subdomain>"
  }

  // Any static environment variables
  environment = {
    PORT = 5000 // Defaults to 80
  }

  // The GitHub repository that hosts the application code
  github_repository = "<organisation>/<repository>"

  // Set to true if the GitHub repository contains both an
  // application and a background task.
  github_repository_dual = false

  // The application healthcheck path and timeout
  healthcheck = {
    endpoint = "/"
    timeout = "30"
  }

  // Minimum and maximum number of application instances
  // The auto-scaling configuration scales the initial
  // number on 60% CPU Utilisation
  instances = {
    minimum = 1
    maximum = 2
    initial = 1
  }

  // The resources needed per application instance
  resources = {
    memory      = 1024
    cpu_credits = 20
  }
}
```

* Add a new application block in `main_apps.tf`, replacing attributes as required.

```hcl
module "application_<name>" {
  source = "./modules/application"

  // Application name
  name                   = "<name>"

  // The environment this application is deployed to (do not change)
  stage                  = var.stage

  // All resources created by this module with have their names
  // prefixed with the string here.
  resource_prefix        = "${var.stage}-<name>"

  // The ECS cluster this application is deployed to
  cluster                = module.cluster.cluster
  // Tags associated with
  tags                   = module.tags.computed
  // The environment variables to expose to the application
  environment            = merge(
    module.storage.environment,
    local.applications["<name>"].environment,
  )
  // The stored secrets to inject into the application environment
  secrets                = module.storage.secrets
  // var.deployment_approval set to a list of users will enforce approvals
  //   to deploy to this environment
  deployment_approval    = var.deployment_approval

  // These are set via the `locals.application_defaults` section in `variables.tf`
  github_repository      = local.applications["<name>"].github_repository
  github_repository_dual = local.applications["<name>"].github_repository_dual
  healthcheck            = local.applications["<name>"].healthcheck
  instances              = local.applications["<name>"].instances
  resources              = local.applications["<name>"].resources
  access                 = merge(
    local.applications["<name>"].access,
    {
      certificate_arn = module.domain.certificate_arn
      zone_id         = module.domain.zone_id
    }
  )

  // EFS volumes to mount for this application
  //   (must set network mode to awsvpc)
  volumes                = {
    datasets = merge(
      module.storage.files.datasets,
      // Add this if you wish the volume to be mounted read-only
      {
          readonly = true
      }
    )
  }

  // Additional IAM permissions, see `outputs.tf` in `modules/storage`
  //   for a list of permissions
  additional_permissions = merge(module.storage.permissions.efs_read)

  network = {
    // Set to awsvpc if you need to mount a volume, bridge otherwise
    mode            = "awsvpc"
    vpc_id          = module.network.vpc_id
    subnets         = module.network.subnet_public_ids
    security_groups = [module.network.security_group_public_id]
  }
}
```

* Run terraform apply
* Check that the service is running on the appropriate domain name, if it is you will see a holding page.
* Update the slackbot token at `/<environment>-<application>/notifications/slack-token` in AWS SSM
* If configured with a GitHub repository add a deployment workflow to the repo,
    otherwise you need only authenticate using the details under `/<environment>-<name>/deployer/access_*`
    and push a built docker image to the

**GitHub deployment workflow (Application Only)**

```yaml
name: Deploy
on:
  push:
    branches: [main]
  workflow_dispatch:
    inputs:
      environment:
        type: environment
        description: The environment to deploy to.

jobs:
  # Include any tests or audits in jobs prior to detect-environments.
  detect-environments:
    runs-on: ubuntu-latest
    needs: [] # Add any previous jobs here
    outputs:
      environments: ${{ steps.environments.outputs.result }}
    steps:
      - uses: actions/github-script@v6
        id: environments
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          result-encoding: json
          script: |
            if (context.payload?.inputs?.environment) return [context.payload?.inputs?.environment];
            const {data: {environments}} =
              await github.request(`GET /repos/${process.env.GITHUB_REPOSITORY}/environments`);
            return environments.map(e => e.name)

  deploy:
    runs-on: ubuntu-latest
    env:
      DOCKER_REPO: ${{ secrets.DEPLOY_DOCKER_REPOSITORY }}
    needs: [detect-environments]
    strategy:
      matrix:
        environment: ${{ fromJSON(needs.detect-environments.outputs.environments) }}
    environment: ${{ matrix.environment }}
    steps:
      - uses: actions/checkout@v3
      - id: vars
        run: echo "sha_short=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT
      - run: |
          sudo apt-get update
          sudo apt-get install -y rsync
      - uses: aws-actions/configure-aws-credentials@v1-node16
        with:
          aws-access-key-id: ${{ secrets.DEPLOY_AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.DEPLOY_AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-west-2
      - uses: aws-actions/amazon-ecr-login@v1
      - run: docker pull $DOCKER_REPO:latest
      - run: docker build -t $DOCKER_REPO:${{ steps.vars.outputs.sha_short }} .
      - run: docker tag $DOCKER_REPO:${{ steps.vars.outputs.sha_short }} $DOCKER_REPO:latest
      - run: docker push $DOCKER_REPO:${{ steps.vars.outputs.sha_short }}
      - run: docker push $DOCKER_REPO:latest
```

### Add a new background task

To create a new background task in the terraform configuration, you will need to know the following:

* The **GitHub repository** (under the [digital-land](https://github.com/digital-land) organisation) hosting the task code.
* A list of events that should trigger a background task run (currently only s3 events are supported).
* An estimate of the **memory usage** of the task.
* A list of the task **environment variables**.
* If the task needs access to the SQLite or MBTiles files.

Add a new section to `variables.tf` under `locals.task_defaults`, replacing attributes as required.

```hcl
<name> = {
  // The GitHub repository that hosts the background task code
  github_repository = "<organisation>/<repository>"

  // Set to true if the GitHub repository contains both a
  // background task and an application.
  github_repository_dual = false

  // The resources needed per background task invocation
  resources = {
    memory      = 8192
    cpu_credits = 1024
  }

  // A map of the events that should trigger this task
  triggers = {
    entity-uploaded = {
      // The events that trigger the task
      events = ["PutObject", "CompleteMultipartUpload"]
      // The S3 bucket to watch for events
      bucket = "${var.stage}-collection-data"
      // The keys of S3 objects to watch for
      watch  = ["/entity-builder/dataset/entity.sqlite3"]
    }
  }
}
```

* Add a new task block in `main_tasks.tf`, replacing attributes as required.

```hcl
module "task_<name>" {
  source = "./modules/application-task"

  // Application name
  name                   = "<name>"

  // The environment this task is deployed to (do not change)
  stage                  = var.stage

  // All resources created by this module with have their names
  // prefixed with the string here.
  resource_prefix        = "${var.stage}-<name>"

  // The ECS cluster this task is deployed to
  cluster                = module.cluster.cluster
  // Tags associated with
  tags                   = module.tags.computed
  // The environment variables to expose to the task
  environment            = merge(
    module.storage.environment,
    local.tasks["<name>"].environment,
  )
  // The stored secrets to inject into the task environment
  secrets                = module.storage.secrets
  // var.deployment_approval set to a list of users will enforce approvals
  //   to deploy to this environment
  deployment_approval    = var.deployment_approval

  // These are set via the `locals.task_defaults` section in `variables.tf`
  github_repository      = local.tasks["<name>"].github_repository
  github_repository_dual = local.tasks["<name>"].github_repository_dual
  triggers               = local.tasks["<name>"].triggers
  instances              = local.tasks["<name>"].instances
  resources              = local.tasks["<name>"].resources

  // EFS volumes to mount for this task
  volumes                = {
    datasets = merge(
      module.storage.files.datasets,
      // Add this if you wish the volume to be mounted read-only
      {
          readonly = true
      }
    )
  }

  // Additional IAM permissions, see `outputs.tf` in `modules/storage`
  //   for a list of permissions
  additional_permissions = merge(module.storage.permissions.efs_read)

  network = {
    vpc_id          = module.network.vpc_id
    subnets         = module.network.subnet_public_ids
    security_groups = [module.network.security_group_public_id]
  }
}
```

* Run terraform apply
* If configured with a GitHub repository add a deployment workflow to the repo,
    otherwise you need only authenticate using the details under `/<environment>-<name>/deployer/access_*`
    and push a built docker image to the

**GitHub deployment workflow (Task Only)**

```yaml
name: Deploy
on:
  push:
    branches: [main]
  workflow_dispatch:
    inputs:
      environment:
        type: environment
        description: The environment to deploy to.

jobs:
  detect-environments:
    runs-on: ubuntu-latest
    outputs:
      environments: ${{ steps.environments.outputs.result }}
    steps:
      - uses: actions/github-script@v6
        id: environments
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          result-encoding: json
          script: |
            if (context.payload?.inputs?.environment) return [context.payload?.inputs?.environment];
            const {data: {environments}} =
              await github.request(`GET /repos/${process.env.GITHUB_REPOSITORY}/environments`);
            return environments.map(e => e.name)

  deploy:
    runs-on: ubuntu-latest
    env:
      DOCKER_REPO: ${{ secrets.DEPLOY_DOCKER_REPOSITORY }}
    needs: [detect-environments]
    strategy:
      matrix:
        environment: ${{ fromJSON(needs.detect-environments.outputs.environments) }}
    environment: ${{ matrix.environment }}
    steps:
      - uses: actions/checkout@v3
      - id: vars
        run: echo "sha_short=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT
      - run: |
          sudo apt-get update
          sudo apt-get install -y rsync
      - uses: aws-actions/configure-aws-credentials@v1-node16
        with:
          aws-access-key-id: ${{ secrets.DEPLOY_AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.DEPLOY_AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-west-2
      - uses: aws-actions/amazon-ecr-login@v1
      - run: docker pull $DOCKER_REPO:latest
      - run: docker build -t $DOCKER_REPO:${{ steps.vars.outputs.sha_short }} .
      - run: docker tag $DOCKER_REPO:${{ steps.vars.outputs.sha_short }} $DOCKER_REPO:latest
      - run: docker push $DOCKER_REPO:${{ steps.vars.outputs.sha_short }}
      - run: docker push $DOCKER_REPO:latest
```

#### Deploy a Task and Application

To deploy both a task and application from the same repository, use the following two github actions workflows.

**`.github/workflows/deploy-application.yml`**

```yaml
name: Deploy Application

on:
  push:
    branches: [main]
    # Put a path here for the folder to watch for changes
    paths: [application/]
  workflow_dispatch:
    inputs:
      environment:
        type: environment
        description: The environment to deploy to (select an -application environment).

jobs:
  detect-environments:
    runs-on: ubuntu-latest
    outputs:
      environments: ${{ steps.environments.outputs.result }}
    steps:
      - uses: actions/github-script@v6
        id: environments
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          result-encoding: json
          script: |
            if (context.payload?.inputs?.environment) return [context.payload?.inputs?.environment];
            const {data: {environments}} =
              await github.request(`GET /repos/${process.env.GITHUB_REPOSITORY}/environments`);
            return environments.map(e => e.name).filter(e => e.indexOf('-application') != -1)

  deploy:
    runs-on: ubuntu-latest
    env:
      DOCKER_REPO: ${{ secrets.DEPLOY_DOCKER_REPOSITORY }}
    needs: [ detect-environments ]
    strategy:
      matrix:
        environment: ${{ fromJSON(needs.detect-environments.outputs.environments) }}
    environment: ${{ matrix.environment }}
    steps:
      - uses: actions/checkout@v3

      - id: vars
        run: echo "sha_short=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT

      - run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip -q awscliv2.zip
          sudo ./aws/install --update
          sudo apt-get update
          sudo apt-get install -y rsync

      - uses: aws-actions/configure-aws-credentials@v1-node16
        with:
          aws-access-key-id: ${{ secrets.DEPLOY_AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.DEPLOY_AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-west-2

      - uses: aws-actions/amazon-ecr-login@v1

      - run: docker pull $DOCKER_REPO:latest || echo "no current latest image"

      - run: docker build -t $DOCKER_REPO:${{ steps.vars.outputs.sha_short }} .
        working-directory: ./application

      - run: docker tag $DOCKER_REPO:${{ steps.vars.outputs.sha_short }} $DOCKER_REPO:latest

      - run: docker push $DOCKER_REPO:${{ steps.vars.outputs.sha_short }}

      - run: docker push $DOCKER_REPO:latest
```

**`.github/workflows/deploy-task.yml`**

```yaml
name: Deploy Task

on:
  push:
    branches: [main]
    # Put a path here for the folder to watch for changes
    paths: [task/]
  workflow_dispatch:
    inputs:
      environment:
        type: environment
        description: The environment to deploy to (select an -application environment).

jobs:
  detect-environments:
    runs-on: ubuntu-latest
    outputs:
      environments: ${{ steps.environments.outputs.result }}
    steps:
      - uses: actions/github-script@v6
        id: environments
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          result-encoding: json
          script: |
            if (context.payload?.inputs?.environment) return [context.payload?.inputs?.environment];
            const {data: {environments}} =
              await github.request(`GET /repos/${process.env.GITHUB_REPOSITORY}/environments`);
            return environments.map(e => e.name).filter(e => e.indexOf('-application') != -1)

  deploy:
    runs-on: ubuntu-latest
    env:
      DOCKER_REPO: ${{ secrets.DEPLOY_DOCKER_REPOSITORY }}
    needs: [ detect-environments ]
    strategy:
      matrix:
        environment: ${{ fromJSON(needs.detect-environments.outputs.environments) }}
    environment: ${{ matrix.environment }}
    steps:
      - uses: actions/checkout@v3

      - id: vars
        run: echo "sha_short=$(git rev-parse --short HEAD)" >> $GITHUB_OUTPUT

      - run: |
          curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
          unzip -q awscliv2.zip
          sudo ./aws/install --update
          sudo apt-get update
          sudo apt-get install -y rsync

      - uses: aws-actions/configure-aws-credentials@v1-node16
        with:
          aws-access-key-id: ${{ secrets.DEPLOY_AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.DEPLOY_AWS_SECRET_ACCESS_KEY }}
          aws-region: eu-west-2

      - uses: aws-actions/amazon-ecr-login@v1

      - run: docker pull $DOCKER_REPO:latest || echo "no current latest image"

      - run: docker build -t $DOCKER_REPO:${{ steps.vars.outputs.sha_short }} .
        working-directory: ./task

      - run: docker tag $DOCKER_REPO:${{ steps.vars.outputs.sha_short }} $DOCKER_REPO:latest

      - run: docker push $DOCKER_REPO:${{ steps.vars.outputs.sha_short }}

      - run: docker push $DOCKER_REPO:latest
```
