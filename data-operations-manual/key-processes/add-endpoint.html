<!doctype html>
<html lang="en" class="govuk-template no-js">
  <head>
    <meta content="IE=edge" http-equiv="X-UA-Compatible">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">

    <title>Data Operations Manual - Key Processes - Adding An Endpoint - Planning Data</title>

    <link href="../../stylesheets/manifest.css" rel="stylesheet" />

    <link rel="canonical" href="digital-land.github.io/data-operations-manual/key-processes/add-endpoint.html">

      <meta name="robots" content="noindex" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:image" content="digital-land.github.io/images/govuk-large.png" />
      <meta name="twitter:title" content="Data Operations Manual - Key Processes - Adding An Endpoint - Planning Data" />
      <meta name="twitter:url" content="digital-land.github.io/data-operations-manual/key-processes/add-endpoint.html" />

      <meta property="og:image" content="digital-land.github.io/images/govuk-large.png" />
      <meta property="og:site_name" content="Planning Data" />
      <meta property="og:title" content="Data Operations Manual - Key Processes - Adding An Endpoint" />
      <meta property="og:type" content="object" />
      <meta property="og:url" content="digital-land.github.io/data-operations-manual/key-processes/add-endpoint.html" />

    
  </head>

  <body class="govuk-template__body">
    <script>document.body.className = ((document.body.className) ? document.body.className + ' js-enabled' : 'js-enabled');</script>

    <div class="app-pane">
      <div class="app-pane__header toc-open-disabled">
        <a href="#content" class="govuk-skip-link" data-module="govuk-skip-link">Skip to main content</a>

        <header class="govuk-header app-header" role="banner" data-module="govuk-header">
  <div class="govuk-header__container govuk-header__container--full-width">
    <div class="govuk-header__logo">
      <a href="https://planning.data.gov.uk" class="govuk-header__link govuk-header__link--homepage">
        <span class="govuk-header__product-name">
          Planning Data
        </span>
      </a>
        <strong class="govuk-tag">Beta</strong>
    </div>
  </div>
</header>

      </div>

        <div id="toc-heading" class="toc-show fixedsticky">
          <button type="button" class="toc-show__label js-toc-show" aria-controls="toc">
            Table of contents <span class="toc-show__icon"></span>
          </button>
        </div>

      <div class="app-pane__body" data-module="in-page-navigation">
          <div class="app-pane__toc">
            <div class="toc" data-module="table-of-contents" tabindex="-1" aria-label="Table of contents" role="dialog">
              <div class="search" data-module="search" data-path-to-site-root="../../">
  <form action="https://www.google.co.uk/search" method="get" role="search" class="search__form govuk-!-margin-bottom-4">
    <input type="hidden" name="as_sitesearch" value="digital-land.github.io"/>
    <label class="govuk-label search__label" for="search" aria-hidden="true">
      Search (via Google)
    </label>
    <input
      type="text"
      id="search" name="q"
      class="govuk-input govuk-!-margin-bottom-0 search__input"
      aria-controls="search-results"
      placeholder="Search">
    <button type="submit" class="search__button">Search</button>
  </form>
</div>

              <button type="button" class="toc__close js-toc-close" aria-controls="toc" aria-label="Hide table of contents"></button>
              <nav id="toc" class="js-toc-list toc__list" aria-labelledby="toc-heading" data-module="collapsible-navigation">
                      <ul>
  <li>
    <a href="../../index.html"><span>Planning Data Service</span></a>
  </li>
  <li>
    <a href="../../documentation/index.html"><span>Documentation</span></a>
  </li>
<li><a href="../../architecture/index.html"><span>Architecture</span></a>
<ul>
  <li>
    <a href="../../architecture/decision-records/index.html"><span>Architecture Decision Records (ADRs)</span></a>
  </li>
<li><a href="../../architecture/design/index.html"><span>Solution design index</span></a>
<ul>
<li><a href="../../architecture/design/latest/index.html"><span>Solution design</span></a>
<ul>
  <li>
    <a href="../../architecture/design/latest/data-pipelines/index.html"><span>Solution design - Data Pipelines</span></a>
  </li>
  <li>
    <a href="../../architecture/design/latest/planning-data-platform/index.html"><span>Solution design - Planning Data Platform</span></a>
  </li>
  <li>
    <a href="../../architecture/design/latest/publish-service/index.html"><span>Solution design - Publish Service</span></a>
  </li>
</ul>
</li>
<li><a href="../../architecture/design/archive/index.html"><span>Solution design archive</span></a>
<ul>
  <li>
    <a href="../../architecture/design/archive/2022-11/index.html"><span>Solution design - November 2022</span></a>
  </li>
</ul>
</li>
<li><a href="../../architecture/design/proposals/index.html"><span>Open Design Proposals</span></a>
<ul>
  <li>
    <a href="../../architecture/design/proposals/template.html"><span>Open Design Proposal - 00X - Title (Template)</span></a>
  </li>
  <li>
    <a href="../../architecture/design/proposals/001-publish-async/index.html"><span>Open Design Proposal 001 - Publish service - Async</span></a>
  </li>
  <li>
    <a href="../../architecture/design/proposals/002-data-pipelines-migration/index.html"><span>Open Design Proposal 002 - Data Pipelines Migration</span></a>
  </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
  <li>
    <a href="../../infrastructure.html"><span>Infrastructure</span></a>
  </li>
<li><a href="../../data-operations-manual/index.html"><span>Data Operations Manual</span></a>
<ul>
<li><a href="../../data-operations-manual/key-processes/index.html"><span>Key Processes</span></a>
<ul>
  <li>
    <a href="../../data-operations-manual/key-processes/validate-endpoint.html"><span>Validating an Endpoint</span></a>
  </li>
  <li>
    <a href="../../data-operations-manual/key-processes/add-endpoint.html"><span>Adding An Endpoint</span></a>
  </li>
  <li>
    <a href="../../data-operations-manual/key-processes/add-collection.html"><span>Creating A Collection</span></a>
  </li>
  <li>
    <a href="../../data-operations-manual/key-processes/add-dataset.html"><span>Adding A Dataset</span></a>
  </li>
  <li>
    <a href="../../data-operations-manual/key-processes/end-endpoint.html"><span>Ending An Endpoint</span></a>
  </li>
  <li>
    <a href="../../data-operations-manual/key-processes/retire-resource.html"><span>Retiring A Resource</span></a>
  </li>
  <li>
    <a href="../../data-operations-manual/key-processes/assign-entities.html"><span>Assigning Entities</span></a>
  </li>
  <li>
    <a href="../../data-operations-manual/key-processes/merge-entities.html"><span>Merging Entities</span></a>
  </li>
</ul>
</li>
  <li>
    <a href="../../data-operations-manual/key-concepts/index.html"><span>Key Concepts</span></a>
  </li>
  <li>
    <a href="../../data-operations-manual/reference/index.html"><span>Reference</span></a>
  </li>
</ul>
</li>
<li><a href="../../data-operations-manual-for-config/index.html"><span>Data Operations Manual For Config</span></a>
<ul>
<li><a href="../../data-operations-manual-for-config/key-processes/index.html"><span>Key Processes</span></a>
<ul>
  <li>
    <a href="../../data-operations-manual-for-config/key-processes/config-add-endpoint.html"><span>Adding An Endpoint</span></a>
  </li>
  <li>
    <a href="../../data-operations-manual-for-config/key-processes/config-add-collection.html"><span>Creating A Collection</span></a>
  </li>
  <li>
    <a href="../../data-operations-manual-for-config/key-processes/config-add-dataset.html"><span>Adding A Dataset</span></a>
  </li>
  <li>
    <a href="../../data-operations-manual-for-config/key-processes/config-end-endpoint.html"><span>Ending An Endpoint</span></a>
  </li>
  <li>
    <a href="../../data-operations-manual-for-config/key-processes/config-retire-resource.html"><span>Retiring A Resource</span></a>
  </li>
  <li>
    <a href="../../data-operations-manual-for-config/key-processes/config-assign-entities.html"><span>Assigning Entities</span></a>
  </li>
</ul>
</li>
</ul>
</li>
  <li>
    <a href="../../runbook.html"><span>Run Book</span></a>
  </li>
  <li>
    <a href="../../WorkingWithLPA_GIS.html"><span>Working with LPA GIS Systems</span></a>
  </li>
  <li>
    <a href="../../HowTos.html"><span>How-to Guides</span></a>
  </li>
</ul>


              </nav>
            </div>
          </div>

        <div class="app-pane__content toc-open-disabled" aria-label="Content" tabindex="0">
          <main id="content" class="technical-documentation" data-module="anchored-headings">
            <h1 id="adding-an-endpoint">Adding An Endpoint</h1>
<h3 id="checking-data-before-loading-it-onto-the-platform-with-endpoint-checker">Checking Data before loading it onto the platform with Endpoint Checker</h3>
<p>The <a href="https://github.com/digital-land/jupyter-analysis/tree/main/endpoint_checker">Endpoint Checker</a> is a Jupyter notebook that you can use to get an idea of the level of usability of the data offered on the web by an LPA. The notebook summarises all the details related to an endpoint in one place, and is a good first place to look when validating a new endpoint.</p>
<h4 id="pre-requisites">pre-requisites</h4>
<p>You need to know</p>

<ul>
<li>The collection you are checking against</li>
<li>The organisation code for the <a href="https://datasette.planning.data.gov.uk/digital-land?sql=select%0D%0A++entity%2C%0D%0A++name%2C%0D%0A++organisation%2C%0D%0A++website%0D%0Afrom%0D%0A++organisation%0D%0Aorder+by%0D%0A++organisation%0D%0Alimit%0D%0A++1000">data publisher</a></li>
<li>The URL where their documentation is (optional but <strong>highly</strong> recommended)</li>
<li>The URL where the data lives</li>
<li>what data it is you are getting</li>
<li>The start_date for this data. This can be in the future, for checking at least.</li>
</ul>
<p>If the file is a CSV, download it first and have a look to make sure that it meets the basic sanity-checks:</p>

<ul>
<li>Is there a reference value for every row?</li>
<li>Are the mandatory fields for that Collection provided?</li>
<li>Are there any formatting problems, like multi-line fields (eg addresses) or misplaced commas?</li>
<li>Are there any rows at the end of the data that are going to cause problems? There is sometimes text or other content that isn&rsquo;t going to work for us.</li>
</ul>
<p>If the link is for a geographic system, again download it and have a look at the data to ensure that there&rsquo;s a reference field and that the mandatory fields are present. You can use the guidance in <a href="WorkingWithLPA_GIS.html">Working with LPA GIS Systems</a> to help deal with any issues. A common one is that the publisher might provide a query with a list of output fields. These have commas in so tends to break the import process. You can usually safely replace all the fields with a single * character to get all the fields.</p>
<p>The endpoint checker gives you a lot of information about the chances of successfully loading the data. I suggest you pick some URLs from existing endpoint.csv files and try them out to get a flavour of what it does but in general you will see</p>

<ol>
<li>Can we get the data downloaded? If so, you&rsquo;ll see information about the size of the download etc</li>
<li>Can we convert it to a CSV structure?</li>
<li>Can we match the columns provided with what we expect according to the standard for that Collection? In the past we have been quite lenient with column mapping for LPAs. As we move towards a more standardised approach, publishers should be using the standards.</li>
<li>Does our system generate lookup values that make sense for the data given?</li>
<li>Are we seeing the right number of rows being brought in vs the original input?</li>
<li>Does the final representation of what the imported data look right? </li>
</ol>
<p>If the data passes those tests you can move on to loading it onto the platform, below.</p>
<h3 id="loading-data-onto-the-platform">Loading data onto the platform</h3>
<p>If you&rsquo;re happy with the results of validating your new endpoint you can use the final part of the notebook to find some convenient scripts to help set up the new download.</p>

<ol>
<li>Download the repository for the Collection you need to add to, or get the latest from it</li>
<li>Set up and activate your virtual environment for it</li>
<li>Run <code>make makerules</code> and <code>make init</code></li>
<li>Create an <code>import.csv</code> file per the notes in the Jupyter notebook</li>
<li>Copy the two lines from the notebook into the file</li>
<li>Save it and run the command line example from the notebook, in the Collection repository</li>
<li>Run <code>make</code> to run the pipeline locally.</li>
</ol>
<p>Depending on the amount of data present in the Collection, the import process may take some time. Once it completes, review the <code>source.csv</code>, <code>endpoint.csv</code> and <code>lookup.csv</code> files to make sure they make sense.</p>
<h4 id="confirming-it-all-worked">Confirming it all worked</h4>
<p>The <code>make</code> process generates some files in the <code>dataset</code> folder in the Collection. You can examine these files for problem in the &lsquo;-issue.csv&rsquo; file and see what waa generated in the other csv file. You might need to use notepad++ to open these files as they can be tens of megabytes in size. You can also open the SQLITE file using and application called &lsquo;DB Browser for SQLite&rsquo;, though you may need to coy the SQLITE file locally before you can open it.</p>
<h4 id="saving-changes-back">Saving changes back</h4>
<p>You can push the the <code>source.csv</code>, <code>endpoint.csv</code> and <code>lookup.csv</code> files files back to github, alongside the <code>import.csv</code>, having discarded any changes to other files not needed but changed by the process. It is a good idea to name the commit after the organisation you are importing. I&rsquo;ve been using the name of the organisation plus the three letter code.</p>
<h3 id="adding-endpoints-to-a-collection-manually">Adding Endpoints To A Collection Manually</h3>
<p>This is a fairly common process for us especially as we initially grow the data on the site.</p>
<h4 id="1-checkout-repo">1. Checkout repo</h4>
<p>From Github, clone the collection repo you wish to add endpoints to.</p>
<h4 id="2-run-collection-init">2. run collection init</h4>
<p>Run the following lines from a virtual env containing the required
python dependencies required by the collection repo</p>
<div class="highlight"><pre class="highlight shell" tabindex="0"><code>make makerules
make init
</code></pre></div><h4 id="3-get-your-csv-of-endpoints-to-add">3. get your csv of endpoints to add</h4>
<p>Find or generate a csv file of the entries that need to be added
to the collection. Make a note of the location. The following columns need to be included in the csv:</p>

<ul>
<li><code>endpoint-url</code> - the url that the collector needs to extract data from</li>
<li><code>documentation-url</code> - a url on a government related domain which contain information regarding the data</li>
<li><code>start-date</code> - the date that the collector should start from (this can be in the past)</li>
<li><code>plugins</code> - if a plugin is required to extract that data then it can be noted here otherwise leave blank</li>
<li><code>pipelines</code> - the pipelines that need to be ran on resources collected from this endpoint. These are equivalent to the datasets and where more than one is necessary they should be separated by <code>;</code></li>
<li><code>organisation</code> - the organisation which the endpoint belongs to. The name should be in <a href="https://datasette.planning.data.gov.uk/digital-land/organisation">this list</a></li>
</ul>
<p>If you haven&rsquo;t been given some of the above information then reach out to the data manager or tech lead for advice. You can use the below to get you started:</p>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>organisation,endpoint-url,documentation-url,start-date,pipelines,plugin
</code></pre></div><p>For example</p>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>organisation,endpoint-url,documentation-url,start-date,pipelines,plugin
local-authority-eng:GRT,https://www.guildford.gov.uk/media/35723/Guildford-Brownfield-Register-2023/CSV/guildford_brownfieldregister_2023_10_12_rev1.csv?m=638327116385130000,https://www.guildford.gov.uk/article/25473/What-is-the-register,2023-10-12,brownfield-land,
</code></pre></div><h4 id="4-run-add-endpoint-and-lookups-cmd">4. run <code>add_endpoint_and_lookups_cmd</code></h4>
<p>Run the following command from inside the repository with the virtual env created in step 2 activated.</p>
<p>As a minimum you&rsquo;ll need to include the path to the input csv generated in step 3. and the name of the collection that your working in.</p>
<p>Note: collection name can be retrieved from the repository name. if your working in the <code>conservation-area-collection</code> repo then you would use <code>conservation-area</code></p>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>digital_land add-endpoints-and-lookups [INPUT-CSV-PATH] [COLLECTION_NAME]
</code></pre></div><p>The above command will work for the default collection set-up however you can set set the location
of the key dependencies if any of the directories or files are in a different location. This should not be necessary most of the time.</p>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>-c, --collection-dir [COLLECTION-DIRECTORY; default="/collection"] The directory containing the collection configuration files, collection logs and resources
-o, --organisation-path [ORGANISATION-CSV-PATH; default="var/cache/organisation.csv"] The path to the csv containing the list of organisations
-s, --specification-dir [SPECIFICATION-DIR; default="/specification"] The directory containing the specification files
-p, --pipeline-dir [PIPELINE-DIRECTORY; default="/pipeline"] The directory containing pipeline configuration files
</code></pre></div><h4 id="5-check-assigned-entities-are-normal">5. check assigned entities are normal</h4>
<p>After running the command, the following Collection repo
files will be modified:</p>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>collection/source.csv
collection/endpoints.csv
pipeline/lookup.csv
</code></pre></div><p>The console output will show a list of new lookup entries
organised by organisation and resource-hash.
E.g.</p>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>----------------------------------------------------------------------
&gt;&gt;&gt; organisations:['local-authority-eng:ARU']
&gt;&gt;&gt; resource:6c38cd1f84054051ca200d62e9715be0cd739bedbae0db9561ef091fa95f59f1
----------------------------------------------------------------------
brownfield-land,,local-authority-eng:ARU,BR23911,1729345
brownfield-land,,local-authority-eng:ARU,BR19811,1729346
...
</code></pre></div><p>Find the first lookup entry in the console output,
make a note of the entity id (the number at the end),
and find this in pipeline/lookup.csv.</p>
<p>Check this and all subsequent lines in lookup.csv for
any anomalies. Each record should have, as a minimum, a prefix,
organisation and reference.</p>
<h4 id="6-run-pipeline">6. run pipeline</h4>
<p>Run the following line from a virtual env containing the required
python dependencies required by the collection repo</p>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>make
</code></pre></div><h4 id="7-perform-final-checks-of-the-data-for-any-anomalies">7. perform final checks of the data for any anomalies</h4>
<p>How this step is performed will largely rely on the level of the operator&rsquo;s
system knowledge.
One approach might be to identify a new (or newly updated) entry in the
.csv file, and search for this entry in the digital_land website.
Navigating through to the Entity screen will show information that may
be compared to the data in the collection/source.csv</p>
<p>Once the pipeline is ran you can also use the following command to interrogate
the local datasets (in the sqlite files) using datasette</p>

            
          </main>

          <aside>
          </aside>

          <footer class="govuk-footer app-footer" role="contentinfo">
  <div class="govuk-footer__meta">
    <div class="govuk-footer__meta-item govuk-footer__meta-item--grow">


      <svg
        aria-hidden="true"
        focusable="false"
        class="govuk-footer__licence-logo"
        xmlns="http://www.w3.org/2000/svg"
        viewbox="0 0 483.2 195.7"
        height="17"
        width="41"
      >
        <path
          fill="currentColor"
          d="M421.5 142.8V.1l-50.7 32.3v161.1h112.4v-50.7zm-122.3-9.6A47.12 47.12 0 0 1 221 97.8c0-26 21.1-47.1 47.1-47.1 16.7 0 31.4 8.7 39.7 21.8l42.7-27.2A97.63 97.63 0 0 0 268.1 0c-36.5 0-68.3 20.1-85.1 49.7A98 98 0 0 0 97.8 0C43.9 0 0 43.9 0 97.8s43.9 97.8 97.8 97.8c36.5 0 68.3-20.1 85.1-49.7a97.76 97.76 0 0 0 149.6 25.4l19.4 22.2h3v-87.8h-80l24.3 27.5zM97.8 145c-26 0-47.1-21.1-47.1-47.1s21.1-47.1 47.1-47.1 47.2 21 47.2 47S123.8 145 97.8 145"
        />
      </svg>
      <span class="govuk-footer__licence-description">
        All content is available under the
        <a
          class="govuk-footer__link"
          href="https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/"
          rel="license"
        >Open Government Licence v3.0</a>, except where otherwise stated
      </span>
    </div>
    <div class="govuk-footer__meta-item">
      <a
        class="govuk-footer__link govuk-footer__copyright-logo"
        href="https://www.nationalarchives.gov.uk/information-management/re-using-public-sector-information/uk-government-licensing-framework/crown-copyright/"
      >© Crown copyright</a>
    </div>
  </div>
</footer>

        </div>
      </div>
    </div>

    
    <script src="../../javascripts/application.js"></script>
  </body>
</html>
