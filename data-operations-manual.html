<!doctype html>
<html lang="en" class="govuk-template no-js">
  <head>
    <meta content="IE=edge" http-equiv="X-UA-Compatible">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">

    <title>Data Operations Manual - Planning Data</title>

    <link href="stylesheets/manifest.css" rel="stylesheet" />

    <link rel="canonical" href="digital-land.github.io/data-operations-manual.html">

      <meta name="robots" content="noindex" />
      <meta name="twitter:card" content="summary" />
      <meta name="twitter:image" content="digital-land.github.io/images/govuk-large.png" />
      <meta name="twitter:title" content="Data Operations Manual - Planning Data" />
      <meta name="twitter:url" content="digital-land.github.io/data-operations-manual.html" />

      <meta property="og:image" content="digital-land.github.io/images/govuk-large.png" />
      <meta property="og:site_name" content="Planning Data" />
      <meta property="og:title" content="Data Operations Manual" />
      <meta property="og:type" content="object" />
      <meta property="og:url" content="digital-land.github.io/data-operations-manual.html" />

    
  </head>

  <body class="govuk-template__body">
    <script>document.body.className = ((document.body.className) ? document.body.className + ' js-enabled' : 'js-enabled');</script>

    <div class="app-pane">
      <div class="app-pane__header toc-open-disabled">
        <a href="#content" class="govuk-skip-link" data-module="govuk-skip-link">Skip to main content</a>

        <header class="govuk-header app-header" role="banner" data-module="govuk-header">
  <div class="govuk-header__container govuk-header__container--full-width">
    <div class="govuk-header__logo">
      <a href="https://planning.data.gov.uk" class="govuk-header__link govuk-header__link--homepage">
        <span class="govuk-header__product-name">
          Planning Data
        </span>
      </a>
        <strong class="govuk-tag">Beta</strong>
    </div>
  </div>
</header>

      </div>

        <div id="toc-heading" class="toc-show fixedsticky">
          <button type="button" class="toc-show__label js-toc-show" aria-controls="toc">
            Table of contents <span class="toc-show__icon"></span>
          </button>
        </div>

      <div class="app-pane__body" data-module="in-page-navigation">
          <div class="app-pane__toc">
            <div class="toc" data-module="table-of-contents" tabindex="-1" aria-label="Table of contents" role="dialog">
              <div class="search" data-module="search" data-path-to-site-root="./">
  <form action="https://www.google.co.uk/search" method="get" role="search" class="search__form govuk-!-margin-bottom-4">
    <input type="hidden" name="as_sitesearch" value="digital-land.github.io"/>
    <label class="govuk-label search__label" for="search" aria-hidden="true">
      Search (via Google)
    </label>
    <input
      type="text"
      id="search" name="q"
      class="govuk-input govuk-!-margin-bottom-0 search__input"
      aria-controls="search-results"
      placeholder="Search">
    <button type="submit" class="search__button">Search</button>
  </form>
</div>

              <button type="button" class="toc__close js-toc-close" aria-controls="toc" aria-label="Hide table of contents"></button>
              <nav id="toc" class="js-toc-list toc__list" aria-labelledby="toc-heading" data-module="collapsible-navigation">
                      <ul>
  <li>
    <a href="./index.html"><span>Planning Data Service</span></a>
  </li>
  <li>
    <a href="./documentation/index.html"><span>Documentation</span></a>
  </li>
<li><a href="./architecture/index.html"><span>Architecture</span></a>
<ul>
  <li>
    <a href="./architecture/decision-records/index.html"><span>Architecture Decision Records (ADRs)</span></a>
  </li>
<li><a href="./architecture/design/index.html"><span>Solution design index</span></a>
<ul>
<li><a href="./architecture/design/latest/index.html"><span>Solution design</span></a>
<ul>
  <li>
    <a href="./architecture/design/latest/data-pipelines/index.html"><span>Solution design - Data Pipelines</span></a>
  </li>
  <li>
    <a href="./architecture/design/latest/planning-data-platform/index.html"><span>Solution design - Planning Data Platform</span></a>
  </li>
  <li>
    <a href="./architecture/design/latest/publish-service/index.html"><span>Solution design - Publish Service</span></a>
  </li>
</ul>
</li>
<li><a href="./architecture/design/archive/index.html"><span>Solution design archive</span></a>
<ul>
  <li>
    <a href="./architecture/design/archive/2022-11/index.html"><span>Solution design - November 2022</span></a>
  </li>
</ul>
</li>
<li><a href="./architecture/design/proposals/index.html"><span>Open Design Proposals</span></a>
<ul>
  <li>
    <a href="./architecture/design/proposals/template.html"><span>Open Design Proposal - 00X - Title (Template)</span></a>
  </li>
  <li>
    <a href="./architecture/design/proposals/001-publish-async/index.html"><span>Open Design Proposal 001 - Publish service - Async</span></a>
  </li>
  <li>
    <a href="./architecture/design/proposals/002-data-pipelines-migration/index.html"><span>Open Design Proposal 002 - Data Pipelines Migration</span></a>
  </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
  <li>
    <a href="./infrastructure.html"><span>Infrastructure</span></a>
  </li>
  <li>
    <a href="./data-operations-manual.html"><span>Data Operations</span></a>
  </li>
  <li>
    <a href="./runbook.html"><span>Run Book</span></a>
  </li>
  <li>
    <a href="./WorkingWithLPA_GIS.html"><span>Working with LPA GIS Systems</span></a>
  </li>
  <li>
    <a href="./HowTos.html"><span>How-to Guides</span></a>
  </li>
</ul>


              </nav>
            </div>
          </div>

        <div class="app-pane__content toc-open-disabled" aria-label="Content" tabindex="0">
          <main id="content" class="technical-documentation" data-module="anchored-headings">
            <h1 id="data-operations">Data Operations</h1>
<p>We have two main goals:
* To collect data from external organisations published in accordance with data specifications we created,
* and make it available as part of a national dataset.</p>
<p>The planning data service relies upon a set of data workflows to achieve that goal.</p>
<h3 id="audience-for-this-document">Audience for this document</h3>
<p>This document is intended for people who need to run data operations for the Planning Data Platform. It is not intended as a manual for software developers on the project, but it does occasionally stray into the world of programming. This doc links out to repositories on <a href="https://github.com/digital-land">github</a> where we host a mix of code and data.</p>
<h2 id="key-concepts">Key Concepts</h2>
<p>We have a number of different Collections of data, such as <a href="https://digital-land.github.io/specification/specification/conservation-area/">Conservation Areas (CA)</a>. The data and configuration for each Collection is stored in Github. For example, the CA repository is at <a href="https://github.com/digital-land/conservation-area-collection"><code>https://github.com/digital-land/conservation-area-collection</code></a>.</p>
<p>In each Collection there is a <code>collection</code> folder that contains some key files that control the collection of data from LPAs and other publishers. The first two to look at are <code>source.csv</code> and <code>endpoint.csv</code>.</p>
<h3 id="source">Source</h3>
<p>the source.csv file lists all of the publishers we have data for in this Collection:</p>
<div class="table-container">
        <table>
          <tr>
<th>Field</th>
<th>Usage</th>
</tr><tr>
<td>source</td>
<td>Identifier for this row in the file</td>
</tr><tr>
<td>attribution</td>
<td>Statement about the source of the data</td>
</tr><tr>
<td>collection</td>
<td>Repeats the name of the thing we are collecting.</td>
</tr><tr>
<td>documentation-url</td>
<td>The web link to the providers documentation about the data</td>
</tr><tr>
<td>endpoint</td>
<td>A link to a line in the endpoint CSV (see below)</td>
</tr><tr>
<td>licence</td>
<td>The kind of license</td>
</tr><tr>
<td>organisation</td>
<td>A short code linking to the <a href="https://datasette.planning.data.gov.uk/digital-land?sql=select+entity%2C+name%2C+organisation%2C+website+from+organisation+order+by+organisation+limit+100">organisation</a> table</td>
</tr><tr>
<td>pipelines</td>
<td>What specific data we are getting from the endpoint</td>
</tr><tr>
<td>entry-date</td>
<td>When the record was added</td>
</tr><tr>
<td>start-date</td>
<td>When the record becomes active</td>
</tr><tr>
<td>end-date</td>
<td>When the record becomes inactive</td>
</tr>
        </table>
      </div><h3 id="endpoint">Endpoint</h3>
<div class="table-container">
        <table>
          <tr>
<th>Field</th>
<th>Usage</th>
</tr><tr>
<td>endpoint</td>
<td>Identifier for this row in the file</td>
</tr><tr>
<td>endpoint-url</td>
<td>Link to the data itself</td>
</tr><tr>
<td>parameters</td>
<td>(unused?)</td>
</tr><tr>
<td>plugin</td>
<td>We have different plugins for reading different kinds of data.</td>
</tr><tr>
<td>entry-date</td>
<td>The date we added this record to the system</td>
</tr><tr>
<td>start-date</td>
<td>When the download becomes valid</td>
</tr><tr>
<td>end-date</td>
<td>When the download stops being valid</td>
</tr>
        </table>
      </div><p><strong><em>NOTE</em></strong>: The relationship between Source and Endpoint does not follow a traditional entity-relationship 1:M model. Every time we add a new Endpoint for an organisation+Collection, we add a new Source as well, referencing that endpoint.</p>
<h3 id="entity">Entity</h3>
<p>Data gets imported and eventually emerges as an Entity. This is used as a catch-all <a href="https://datasette.planning.data.gov.uk/entity?sql=select+dataset%2C+end_date%2C+entity%2C+entry_date%2C+geojson%2C+geometry%2C+json%2C+name%2C+organisation_entity%2C+point%2C+prefix%2C+reference%2C+start_date%2C+typology%2C+geometry_geom%2C+point_geom+from+entity+where+entity+%3E+12345+order+by+entity+limit+101">record</a> for pretty much everything we hold.</p>
<p>Each entity has a field called <code>entity</code> which is its identifier. Another important field is the <code>reference</code> which is what the data publisher uses to refer to the thing we are holding information about. The rest of the fields act as you might expect with the exception of the entry_date field.</p>

<blockquote>
<p><strong><em>entry_date</em></strong>: Normally in this system, entry_date is the date upon which a record was added to the system. A bit like _<em>created_date</em> in other systems. However, for data we import from a publisher, it can <strong>either</strong> be the date we imported it, <strong>or</strong> a date value provided by the publisher. We have no way of distinguishing between those two cases.</p>
</blockquote>
<h3 id="lookups">Lookups</h3>
<p>Now you know about Entity, we can look at another file, <code>lookup.csv</code> in the <a href="https://github.com/digital-land/conservation-area-collection/blob/main/pipeline/lookup.csv"><code>pipeline</code></a> folder.</p>
<p>There is a complex relationship between Entities we hold. Some entities give us data. Other entities ARE the things you see on the map. Sometimes more than one publisher gives us information about a single thing on the map. To manage this complexity we have a table called Lookup which maps from &lsquo;what the provider calls a thing&rsquo; to our entity.</p>
<p>This relationship occasionally requires manual intervention, but for the time being you just need to understand that allocating &lsquo;entity&rsquo; identifier values for stuff we import is an operation done when we import the data rather than something done by a database.</p>
<div class="table-container">
        <table>
          <tr>
<th>Field</th>
<th>Usage</th>
</tr><tr>
<td>prefix</td>
<td>The dataset this entity is a part of</td>
</tr><tr>
<td>resource</td>
<td>(not used?)</td>
</tr><tr>
<td>organisation</td>
<td>Who has data referring to this entity</td>
</tr><tr>
<td>reference</td>
<td>And what they use as a reference when talking about this entity</td>
</tr><tr>
<td>entity</td>
<td>What <strong>we</strong> use to talk about the entity</td>
</tr>
        </table>
      </div><p>As you can see, this gives us a Many-to-One relationship between organisation and entity.</p>
<h4 id="plugins">Plugins</h4>
<p>Most publishers either give us link to a CSV file or a link to an <code>HTTP get</code> that returns data in a geographical format. The <a href="https://github.com/digital-land/digital-land-python/tree/main/digital_land/plugins">plugin</a> converts the data to a csv format we can use in the rest of the ingest process.</p>
<h2 id="key-processes">Key Processes</h2>
<p>There are two operations we need to do almost every day - checking the acceptability of data on the platform, and actually setting up so that it loads.</p>
<h3 id="checking-data-before-loading-it-onto-the-platform-with-endpoint-checker">Checking Data before loading it onto the platform with Endpoint Checker</h3>
<p>The <a href="https://github.com/digital-land/jupyter-analysis/tree/main/endpoint_checker">Endpoint Checker</a> is a Jupyter notebook that you can use to get an idea of the level of usability of the data offered on the web by an LPA. The notebook summarises all the details related to an endpoint in one place, and is a good first place to look when validating a new endpoint.</p>
<h4 id="pre-requisites">pre-requisites</h4>
<p>You need to know</p>

<ul>
<li>The collection you are checking against</li>
<li>The organisation code for the <a href="https://datasette.planning.data.gov.uk/digital-land?sql=select%0D%0A++entity%2C%0D%0A++name%2C%0D%0A++organisation%2C%0D%0A++website%0D%0Afrom%0D%0A++organisation%0D%0Aorder+by%0D%0A++organisation%0D%0Alimit%0D%0A++1000">data publisher</a></li>
<li>The URL where their documentation is (optional but <strong>highly</strong> recommended)</li>
<li>The URL where the data lives</li>
<li>what data it is you are getting</li>
<li>The start_date for this data. This can be in the future, for checking at least.</li>
</ul>
<p>If the file is a CSV, download it first and have a look to make sure that it meets the basic sanity-checks:</p>

<ul>
<li>Is there a reference value for every row?</li>
<li>Are the mandatory fields for that Collection provided?</li>
<li>Are there any formatting problems, like multi-line fields (eg addresses) or misplaced commas?</li>
<li>Are there any rows at the end of the data that are going to cause problems? There is sometimes text or other content that isn&rsquo;t going to work for us.</li>
</ul>
<p>If the link is for a geographic system, again download it and have a look at the data to ensure that there&rsquo;s a reference field and that the mandatory fields are present. You can use the guidance in <a href="WorkingWithLPA_GIS.html">Working with LPA GIS Systems</a> to help deal with any issues. A common one is that the publisher might provide a query with a list of output fields. These have commas in so tends to break the import process. You can usually safely replace all the fields with a single * character to get all the fields.</p>
<p>The endpoint checker gives you a lot of information about the chances of successfully loading the data. I suggest you pick some URLs from existing endpoint.csv files and try them out to get a flavour of what it does but in general you will see</p>

<ol>
<li>Can we get the data downloaded? If so, you&rsquo;ll see information about the size of the download etc</li>
<li>Can we convert it to a CSV structure?</li>
<li>Can we match the columns provided with what we expect according to the standard for that Collection? In the past we have been quite lenient with column mapping for LPAs. As we move towards a more standardised approach, publishers should be using the standards.</li>
<li>Does our system generate lookup values that make sense for the data given?</li>
<li>Are we seeing the right number of rows being brought in vs the original input?</li>
<li>Does the final representation of what the imported data look right? </li>
</ol>
<p>If the data passes those tests you can move on to loading it onto the platform, below.</p>
<h3 id="loading-data-onto-the-platform">Loading data onto the platform</h3>
<p>If you&rsquo;re happy with the results of validating your new endpoint you can use the final part of the notebook to find some convenient scripts to help set up the new download.</p>

<ol>
<li>Download the repository for the Collection you need to add to, or get the latest from it</li>
<li>Set up and activate your virtual environment for it</li>
<li>Run <code>make makerules</code> and <code>make init</code></li>
<li>Create an <code>import.csv</code> file per the notes in the Jupyter notebook</li>
<li>Copy the two lines from the notebook into the file</li>
<li>Save it and run the command line example from the notebook, in the Collection repository</li>
<li>Run <code>make</code> to run the pipeline locally.</li>
</ol>
<p>Depending on the amount of data present in the Collection, the import process may take some time. Once it completes, review the <code>source.csv</code>, <code>endpoint.csv</code> and <code>lookup.csv</code> files to make sure they make sense.</p>
<h4 id="confirming-it-all-worked">Confirming it all worked</h4>
<p>The <code>make</code> process generates some files in the <code>dataset</code> folder in the Collection. You can examine these files for problem in the &lsquo;-issue.csv&rsquo; file and see what waa generated in the other csv file. You might need to use notepad++ to open these files as they can be tens of megabytes in size. You can also open the SQLITE file using and application called &lsquo;DB Browser for SQLite&rsquo;, though you may need to coy the SQLITE file locally before you can open it.</p>
<h4 id="saving-changes-back">Saving changes back</h4>
<p>You can push the the <code>source.csv</code>, <code>endpoint.csv</code> and <code>lookup.csv</code> files files back to github, alongside the <code>import.csv</code>, having discarded any changes to other files not needed but changed by the process. It is a good idea to name the commit after the organisation you are importing. I&rsquo;ve been using the name of the organisation plus the three letter code.</p>
<h2 id="other-processes">Other Processes</h2>
<p>This is our list of key processes that the team will need to perform to maintain the service. They primarily rely on running CLI tools from our
digital-land-python python package.</p>
<h3 id="adding-endpoints-to-a-collection-manually">Adding Endpoints To A Collection Manually</h3>
<p>This is a fairly common process for us especially as we initially grow the data on the site.</p>
<h4 id="1-checkout-repo">1. Checkout repo</h4>
<p>From Github, clone the collection repo you wish to add endpoints to.</p>
<h4 id="2-run-collection-init">2. run collection init</h4>
<p>Run the following lines from a virtual env containing the required
python dependencies required by the collection repo</p>
<div class="highlight"><pre class="highlight shell" tabindex="0"><code>make makerules
make init
</code></pre></div><h4 id="3-get-your-csv-of-endpoints-to-add">3. get your csv of endpoints to add</h4>
<p>Find or generate a csv file of the entries that need to be added
to the collection. Make a note of the location. The following columns need to be included in the csv:</p>

<ul>
<li><code>endpoint-url</code> - the url that the collector needs to extract data from</li>
<li><code>documentation-url</code> - a url on a government related domain which contain information regarding the data</li>
<li><code>start-date</code> - the date that the collector should start from (this can be in the past)</li>
<li><code>plugins</code> - if a plugin is required to extract that data then it can be noted here otherwise leave blank</li>
<li><code>pipelines</code> - the pipelines that need to be ran on resources collected from this endpoint. These are equivalent to the datasets and where more than one is necessary they should be separated by <code>;</code></li>
<li><code>organisation</code> - the organisation which the endpoint belongs to. The name should be in <a href="https://datasette.planning.data.gov.uk/digital-land/organisation">this list</a></li>
</ul>
<p>If you haven&rsquo;t been given some of the above information then reach out to the data manager or tech lead for advice. You can use the below to get you started:</p>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>organisation,endpoint-url,documentation-url,start-date,pipelines,plugin
</code></pre></div><p>For example</p>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>organisation,endpoint-url,documentation-url,start-date,pipelines,plugin
local-authority-eng:GRT,https://www.guildford.gov.uk/media/35723/Guildford-Brownfield-Register-2023/CSV/guildford_brownfieldregister_2023_10_12_rev1.csv?m=638327116385130000,https://www.guildford.gov.uk/article/25473/What-is-the-register,2023-10-12,brownfield-land,
</code></pre></div><h4 id="4-run-add-endpoint-and-lookups-cmd">4. run <code>add_endpoint_and_lookups_cmd</code></h4>
<p>Run the following command from inside the repository with the virtual env created in step 2 activated.</p>
<p>As a minimum you&rsquo;ll need to include the path to the input csv generated in step 3. and the name of the collection that your working in.</p>
<p>Note: collection name can be retrieved from the repository name. if your working in the <code>conservation-area-collection</code> repo then you would use <code>conservation-area</code></p>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>digital_land add-endpoints-and-lookups [INPUT-CSV-PATH] [COLLECTION_NAME]
</code></pre></div><p>The above command will work for the default collection set-up however you can set set the location
of the key dependencies if any of the directories or files are in a different location. This should not be necessary most of the time.</p>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>-c, --collection-dir [COLLECTION-DIRECTORY; default="/collection"] The directory containing the collection configuration files, collection logs and resources
-o, --organisation-path [ORGANISATION-CSV-PATH; default="var/cache/organisation.csv"] The path to the csv containing the list of organisations
-s, --specification-dir [SPECIFICATION-DIR; default="/specification"] The directory containing the specification files
-p, --pipeline-dir [PIPELINE-DIRECTORY; default="/pipeline"] The directory containing pipeline configuration files
</code></pre></div><h4 id="5-check-assigned-entities-are-normal">5. check assigned entities are normal</h4>
<p>After running the command, the following Collection repo
files will be modified:</p>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>collection/source.csv
collection/endpoints.csv
pipeline/lookup.csv
</code></pre></div><p>The console output will show a list of new lookup entries
organised by organisation and resource-hash.
E.g.</p>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>----------------------------------------------------------------------
&gt;&gt;&gt; organisations:['local-authority-eng:ARU']
&gt;&gt;&gt; resource:6c38cd1f84054051ca200d62e9715be0cd739bedbae0db9561ef091fa95f59f1
----------------------------------------------------------------------
brownfield-land,,local-authority-eng:ARU,BR23911,1729345
brownfield-land,,local-authority-eng:ARU,BR19811,1729346
...
</code></pre></div><p>Find the first lookup entry in the console output,
make a note of the entity id (the number at the end),
and find this in pipeline/lookup.csv.</p>
<p>Check this and all subsequent lines in lookup.csv for
any anomalies. Each record should have, as a minimum, a prefix,
organisation and reference.</p>
<h4 id="6-run-pipeline">6. run pipeline</h4>
<p>Run the following line from a virtual env containing the required
python dependencies required by the collection repo</p>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>make
</code></pre></div><h4 id="7-perform-final-checks-of-the-data-for-any-anomalies">7. perform final checks of the data for any anomalies</h4>
<p>How this step is performed will largely rely on the level of the operator&rsquo;s
system knowledge.
One approach might be to identify a new (or newly updated) entry in the
.csv file, and search for this entry in the digital_land website.
Navigating through to the Entity screen will show information that may
be compared to the data in the collection/source.csv</p>
<p>Once the pipeline is ran you can also use the following command to interrogate
the local datasets (in the sqlite files) using datasette</p>
<h3 id="creating-a-new-collection">Creating a new collection</h3>
<p>This is needed when adding a dataset that doesn&rsquo;t belong to one of the collections
that already exist.</p>
<h4 id="1-ensure-the-collection-is-noted-in-the-specification">1. Ensure the collection is noted in the specification</h4>
<p>Not doing this won&rsquo;t result in any specific errors but it should be done to ensure that the list of collections in the specification is up to date. This should be done with some visiability from the standards team</p>
<h4 id="2-use-the-collection-template-to-create-a-repository">2. Use the <a href="https://github.com/digital-land/collection-template">collection-template</a> to create a repository</h4>
<p>The <code>collection-template</code> repo is a template repository which can be used to create
new collection repos. This is fairly simple using the Github UI. there is an option
to use the template to create a new repo if you navigate to the <code>collection-template</code>
page</p>
<h4 id="3-update-readme">3. Update <code>.README</code></h4>
<p>Update the details in the readme file.</p>
<h4 id="4-deactivate-actions">4. Deactivate actions</h4>
<p>The collection comes with the action ready to run every night. This should be paused until
your ready for it to be ran. This can be done by commenting out the following lines in the <code>.github
/workflows/run_caller.yaml</code> :</p>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>schedule:
- cron: 0 0 * * *
</code></pre></div><h4 id="5-add-endpoints-edit-pipeline-config-files-and-test-locally">5. Add endpoints, edit pipeline config files and test locally.</h4>
<p>This is where you need to get the pipeline running locally. see our other
processes on this page for how to add data</p>
<h4 id="6-re-activate-actions">6. Re-activate actions</h4>
<p>Once the collection is ready to be ran nightly uncomment the line from above in
the git workflow yaml file and it will start running nightly!</p>

<hr>
<p><strong>Process Review</strong></p>
<p>Everything else is done so it&rsquo;s worth reviewing this process and seeing if there&rsquo;s
anyway we can improve it!</p>

<hr>
<h3 id="adding-a-new-dataset">Adding a new dataset</h3>
<p>These requests will likely be few and far between but will happen and will require
co-ordination between both the operations and standards teams. Requests will likely
come from both the data manager and the standards team.</p>
<h4 id="1-ensure-the-dataset-has-been-added-to-the-specification">1. Ensure the dataset has been added to the specification</h4>
<p>Before resources can be processed for a dataset the dataset itself needs to be registered
in the specification. This includes recording which fields are needed at the end of the pipeline
and the entity number range.</p>
<p>This should be done by the standards team before datasets are requested to come
to the platform but if not it&rsquo;s worth involving them here before you continue.</p>
<p>Note: Don&rsquo;t include a collection in the dataset specification until the dataset set should go
on the site. this will allow you to run the pipeline and push data to s3 without
it being added to the platform</p>
<h4 id="2-identify-or-create-a-collection">2. Identify or create a collection</h4>
<p>A dataset HAS to be added to a collection repository. If there isn&rsquo;t already one
then view the process to add a collection and follow the initial steps.</p>
<h4 id="3-identify-and-add-an-endpoint-for-the-dataset">3. Identify and add an endpoint for the dataset</h4>
<p>Once the above steps have been taken the only thing needed to be done is to add endpoints for the dataset. this can be done using our add endpoints process.</p>
<p>Note: if you are adding a dataset that is &lsquo;hosted&rsquo; by us then it can be put in a data folder of the repo and the raw github link e.g. <code>https://raw.githubusercontent.com/digital-land/ancient-woodland-collection/main/data/ancient-woodland-status.csv</code> can then be used as an endpoint.</p>
<h4 id="4-test-locally-and-in-gha">4. Test locally and in GHA</h4>
<p>it&rsquo;s worth testing the dataset locally and in the GHA. this ensures that everything is working as expected. configuratino files can be altered to ensure it works as expected.</p>
<h4 id="5-add-the-collection-to-the-dataset-in-the-specification">5. Add the collection to the dataset in the specification</h4>
<p>This will signal that the dataset should be loaded into the postgresql DB behind the platform. once this step is done when the collection is ran it&rsquo;ll upload to the platform.</p>

<hr>
<p><strong>Process Review</strong></p>
<p>Everything else is done so it&rsquo;s worth reviewing this process and seeing if there&rsquo;s
anyway we can improve it!</p>

<hr>
<h3 id="ending-an-endpoint">Ending an endpoint</h3>
<p>This is needed whenever an endpoint is consistently failing or an LPA want’s to change to us getting the data from a different source.
this process does not remove the record of an endpoint but it stops the collector from attempting to retrieve new resources from it.</p>
<h4 id="1-identify-and-end-endpoint">1. Identify and end endpoint</h4>
<p>You’ll need to identify the endpoint your looking to remove. The minimum information you’ll need is either the <code>endpoint-url</code> e.g. <code>https://raw.githubusercontent.com/digital-land/ancient-woodland-collection/main/data/ancient-woodland-status.csv</code> or the <code>endpoint</code> (hash) e.g. <code>f22b13c88a6c367b06e4c80aa61cb1620d7ca03193daeb6c5f65d014d16c6d2e</code>
Using either fo these fields you can navigate to the the appropriate endpoint.csv and add an end-date to the line.
The end-date must have the format <code>YYYY-MM-DD</code> otherwise it can cause errors further down the pipeline.
Note: editing the endpoint.csv can either be done by cloning the specific collection repo or by editing it in the browser via github’s ui</p>
<h4 id="2-identify-and-end-sources">2. Identify and end sources</h4>
<p>Sources are dependant on endpoints so they should have end-dates added as well. This can easily be done by locating the relevant lines in the <code>source.csv</code> which have the endpoint hash associated with them.
As above the end-date must have the format <code>YYYY-MM-DD</code> otherwise it can cause errors further down the pipeline.
Note: remember there may be multiple sources for one endpoint!</p>
<h4 id="3-optional-stop-old-resources-processing">3. Optional - Stop old resources processing</h4>
<p>When removing an endpoint there is an important question that needs to be asked. Should we still process resources collected from this endpoint?
The majority of the time the answer to this will be yes. For example if we have collected 2022 brownfield land data for an LPA and we need to change to a 2023 endpoint then we still want to keep the 2022 data in our system.
Identify the resource hashes for the endpoint your removing and follow the retire a resource process1</p>
<h4 id="4-review-this-process-and-documentation">4. Review this process and documentation</h4>
<p>Everything else is done so it’s worth reviewing this process and seeing if there’s anyway we can improve it!</p>
<h3 id="retiring-a-resource">Retiring a resource</h3>
<p>Sometimes a resource should not continue to be processed and included in the platform. this can happen for a few different reasons an example would be that it contains significant errors and therefore should be removed.</p>
<h4 id="1-identify-resource-hashes">1. Identify resource hashes</h4>
<p>In order to remove a resource the main thing you’ll need is the hash value for the resource. There are a number of ways to find this by examining tables in datasette or running <code>make collection</code> and examining the resource.csv</p>
<h4 id="2-add-rows-to-old-resource-csv">2. add rows to <code>old-resource.csv</code></h4>
<p>for each resource you want to retire you’ll need to to create a row in the csv. For each row you’ll need the following information:
* <code>old-resource</code> - the resource hash you identified above
* <code>status</code> - for retiring a resource this should be <code>410</code>
* <code>resource</code> - not needed for retiring an endpoint
* <code>notes</code> - details on why the resource should no longer be processed</p>
<h4 id="retiring-a-resource-4-review-this-process-and-documentation">4. Review this process and documentation</h4>
<p>Everything else is done so it’s worth reviewing this process and seeing if there’s anyway we can improve it!</p>
<h2 id="validating-an-endpoint">Validating an Endpoint</h2>
<p>We offer two complimentary ways of doing this - a quick way using the Endpoint Checker, and a more thorough manual method.</p>
<h3 id="validating-an-endpoint-using-the-command-line">Validating an Endpoint using the command-line</h3>
<p>This gives a fuller view of how the endpoint will be handled by the pipeline.</p>
<h4 id="prerequisites">Prerequisites</h4>

<ol>
<li>Clone the collection repository locally.</li>
<li>Create and activate a virtual environment within the collection folder you just cloned.</li>
<li>Run these commands:</li>
</ol>
<div class="highlight"><pre class="highlight shell" tabindex="0"><code>make makerules
make init
make
</code></pre></div><p>If you&rsquo;re looking at large endpoint, you can speed things up a bit by removing all the folders in collection/log and all entries from collection/endpoint.csv.</p>
<h4 id="validating-an-endpoint-locally">Validating an endpoint locally</h4>
<p>The command takes the form shown below:</p>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>digital-land collection-add-source &lt;collection-name&gt; &lt;endpoint-url&gt; organisation &lt;organisation-name&gt; documentation-url &lt;documentation url&gt;
</code></pre></div><p>All the fields given above are required, as shown in the example below:</p>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>digital-land collection-add-source brownfield-land htts://data.fulchester.gov.uk/brownfield/brownfield20230922.csv organisation &lt;organisation-name&gt; documentation-url &lt;documentation url&gt;
</code></pre></div><p>optional fields:
pipelines <dataset/pipeline> use this when dataset name is different from the collection name
licence <licence>
attribution <attribution></p>
<p>Note :
Organisation-name format - local-authority-eng:ABC
If there is an error saying command not found make sure prerequisite steps are followed.</p>

<ol>
<li>Run pipeline by running the ‘make’ command – this ensures that the resource has been downloaded by the collector</li>
<li>Go to collection/log -&gt; search for endpoint hash you added (can be found from collection/endpoint.csv) -&gt; open file
Make sure the status parameter has value 200 which means that the end endpoint link is valid.
If not then raise this as an issue by creating a new card</li>
</ol>

<hr>
<h3 id="assigning-unknown-entities-existing-endpoint-updated">Assigning unknown entities (Existing endpoint updated)</h3>
<p>For our service to recognise data from a provider, it must be assigned an entity number in the lookup.csv for the collection.
This happens automatically when running the add-endpoints-and-lookups script, but sometimes you may want to run this process by itself.
For instance, if an endpoint already on the system is updated with a new resource with new entries, these new entries will need to be assigned entity numbers.
The command for this process is as follows:</p>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>digital-land assign-entities &lt;path-to-resource&gt; &lt;collection-name&gt;
</code></pre></div><p>For example:
<code>
digital-land assign-entities collection/resource/1b8ee25a45cf6e874e4822f2e9b0e83c01df18a73d962b4d376dae1288018b4c article-4-direction
</code></p>
<p>Note that the resource must have been processed by the pipeline for this process to work.</p>

<hr>
<h3 id="merge-existing-duplicated-entities-by-adding-redirections">Merge existing duplicated entities by adding redirections</h3>
<p>add-directions command redirects given entities using the entity numbers by adding entries in the old-entity.csv file.</p>
<h4 id="1-create-a-csv-file-with-duplicated-entity-numbers">1. create a csv file with duplicated entity numbers</h4>
<p>Find or generate a csv file of the entries that need to be added
to the collection. Make a note of the location. The following columns need to be included in the csv:</p>

<ul>
<li><code>entity_source</code> - entity number to redirect</li>
<li><code>entity_destination</code> - entity number to which the entity_source should point to</li>
</ul>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>entity_source,entity_destination
</code></pre></div><p>For example:</p>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>entity_source,entity_destination
100,200
101,201
</code></pre></div><h4 id="2-run-command">2. run command</h4>
<div class="highlight"><pre class="highlight plaintext" tabindex="0"><code>digital-land add-redirections &lt;csv-file-path&gt;
</code></pre></div><p>For example:
<code>
digital-land add-redirections import.csv
</code></p>
<p>This will add entries in the old-entity.csv file in the required format.
The entity number provided in entity_source will get redirected to entity_destination.</p>

<hr>
<h2 id="reference">Reference</h2>
<p>This section of our documentation provides authoritative information about the software.</p>
<p>NOTE: Collection vs collection. A Collection with a capital C is the thing we are gathering - Tree Preservation Orders is a Collection, for example. When we use the term &lsquo;collection&rsquo; with a small c then we mean the <strong>activity</strong> of getting the data for a Collection. The collector is a term we sometimes use for the part of the software that collects the data.</p>
<h3 id="technical-glossary">Technical Glossary</h3>
<p>We use a lot of terms that are very specific to our system. We have listed some of them below.</p>
<div class="table-container">
        <table>
          <tr>
<th>Term</th>
<th>Definition</th>
</tr><tr>
<td>Collection Repository</td>
<td>This refers to one of our 30+ repositories in GitHub which contain all of the configuration to run collection, usually every night.</td>
</tr><tr>
<td>Collection Directory</td>
<td>The Collection directory contains both the configuration files for a collector to run and also the logs and collected resources from previous collector runs. It is inside the Collection Repository.</td>
</tr><tr>
<td>Resource</td>
<td>//TODO</td>
</tr><tr>
<td>Endpoint</td>
<td>An endpoint is a link to the data from an Organisation within a Collection.</td>
</tr>
        </table>
      </div><h3 id="key-repositories">Key Repositories</h3>
<p>There are a lot of repositories within the project, many of them relating to Collections or to prototypes. The table below lists some of the key ones.</p>
<div class="table-container">
        <table>
          <tr>
<th>Repository</th>
<th>Description</th>
</tr><tr>
<td><a href="https://github.com/digital-land/specification">Specification</a></td>
<td>Contains the key documents that define the data we are collecting.</td>
</tr><tr>
<td><a href="https://github.com/digital-land/digital-land.info">Web Application</a></td>
<td>The Python code for the <a href="https://www.planning.data.gov.uk/">web site</a>.</td>
</tr><tr>
<td>
<a href="https://github.com/digital-land/digital-land-python">Digital Land Pipeline</a> (Digital Land Python)</td>
<td>Python command-line tools for collecting and converting resources into a dataset, code library that supports other applications.</td>
</tr><tr>
<td><a href="https://github.com/digital-land/jupyter-analysis">Digital Jupyter Analysis</a></td>
<td>Pieces of analysis of the platform as jupyter notebooks.</td>
</tr>
        </table>
      </div>
            
          </main>

          <aside>
          </aside>

          <footer class="govuk-footer app-footer" role="contentinfo">
  <div class="govuk-footer__meta">
    <div class="govuk-footer__meta-item govuk-footer__meta-item--grow">


      <svg
        aria-hidden="true"
        focusable="false"
        class="govuk-footer__licence-logo"
        xmlns="http://www.w3.org/2000/svg"
        viewbox="0 0 483.2 195.7"
        height="17"
        width="41"
      >
        <path
          fill="currentColor"
          d="M421.5 142.8V.1l-50.7 32.3v161.1h112.4v-50.7zm-122.3-9.6A47.12 47.12 0 0 1 221 97.8c0-26 21.1-47.1 47.1-47.1 16.7 0 31.4 8.7 39.7 21.8l42.7-27.2A97.63 97.63 0 0 0 268.1 0c-36.5 0-68.3 20.1-85.1 49.7A98 98 0 0 0 97.8 0C43.9 0 0 43.9 0 97.8s43.9 97.8 97.8 97.8c36.5 0 68.3-20.1 85.1-49.7a97.76 97.76 0 0 0 149.6 25.4l19.4 22.2h3v-87.8h-80l24.3 27.5zM97.8 145c-26 0-47.1-21.1-47.1-47.1s21.1-47.1 47.1-47.1 47.2 21 47.2 47S123.8 145 97.8 145"
        />
      </svg>
      <span class="govuk-footer__licence-description">
        All content is available under the
        <a
          class="govuk-footer__link"
          href="https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/"
          rel="license"
        >Open Government Licence v3.0</a>, except where otherwise stated
      </span>
    </div>
    <div class="govuk-footer__meta-item">
      <a
        class="govuk-footer__link govuk-footer__copyright-logo"
        href="https://www.nationalarchives.gov.uk/information-management/re-using-public-sector-information/uk-government-licensing-framework/crown-copyright/"
      >© Crown copyright</a>
    </div>
  </div>
</footer>

        </div>
      </div>
    </div>

    
    <script src="javascripts/application.js"></script>
  </body>
</html>
